--- 
pagetitle: "Week 8 - Modeling data"
---

# Week 8

## Week 8 Learning objectives

:::keyidea

At the end of this lesson you will be able to: 


:::




## A framework for modeling

Statistical modeling and machine learning are often considered the key components of data science. There are entire courses in our department at the Johns Hopkins Bloomberg School of Public Health and across all of academics focused squarely on these topics. We aren't going to try to cover these whole topics in a single lecture! Instead, our focus is to cover the key concepts and ideas behind how you can fit the tools you already know into the data science process we have been learning about over the course of this class. 

In last week's lecture we covered the principles of exploratory data analysis (EDA). The goal of EDA is to familiarize yourself with the structure, quirks, and potential flaws in the data set. The final step is an initial "sketch" for the statistical modeling approach that you plan to use. While it is easier to teach data munging, exploratory data analysis and statistical modeling as separate lectures in a course, the reality is that these components form a tightly interconnected feedback loop. 

The statistical modeling component of this feedback loop focuses on creating a precise quantification of both the signals in the data set and the uncertainty we may have about those signals. To do that we deploy a variety of mathematical models but at the heart of these models is a goal to understand the way the world works. So the mathematical models you use, whether for statistical inference, prediction, or causal inference should be developed with the understanding they are part of the overall arc of the data analytic story. 

There is a [famous phrase](https://en.wikipedia.org/wiki/All_models_are_wrong) in statistics: 

> All models are wrong, some are useful - George Box

Like "correlation does not imply causation" this is a pithy phrase that gets tossed around a lot. But what does it actually mean? It means that when we are doing statistical analysis or machine learning it will be nearly impossible for us to get all of the right variables in the equations in all of the right forms. This means that no matter how hard we try our model will be "wrong". But if we are careful about how we interpret the model - quantifying important trends and documenting artifacts and uncertainty - we can say something about the way the world works. So the model may be "useful". 

It is helpful to remember when performing statistical modeling that the goal isn't to quantify the "truth". The goal is to fairly represent a summary of the trends in the data. 

### Identify your goal

We discussed the different types of statistical questions in the first lecture of this course: 

![](images/week1/questions.png)

You can use statistical models to address any of the types of questions, from descriptions of the data to mechanistic models. However, for this lecture we will  on statistical inference and statistical prediction (sometimes called machine learning). These are the two most popular data analytic tasks; moreover most other types of analysis rely on the same models used for either statistical inference or machine learning with the addition or subtraction of some assumptions. 


#### Statistical inference 

The goal of statistical inference is to report sources of "signal" in a data set, while documenting and accounting for both systematic and "random" sources of errors. Just as there is a [central dogma of molecular biology](https://en.wikipedia.org/wiki/Central_dogma_of_molecular_biology) there is also a central dogma of statistics, which I first saw coined in Josh Akey's [lecture notes](https://www.gs.washington.edu/academics/courses/akey/56008/lecture/lecture1.pdf):

![](images/week8/cdstats.png)

Statistical inference is focused on using probability to sample from a population, take measurements on that sample, and use the samples to infer something about the characteristics of the population _on average_. 

The characteristic of the population you are estimating is called a _parameter_ and you use a _statistical estimate_ to try to guess what that parameter might be. You can then use the information you have about sources of uncertainty to infer how accurate and precise you think your estimate will be. 

#### Machine learning

The goal of machine learning is to use a data set to create a prediction function that can be used to predict a new value of the outcome on the basis of a set of input features. 

![](images/week8/cdml.png)

The central dogma of machine learning is similar to the central dogma of statistics in the sense that you are performing a statistical calculation the basis of some observed data. However, the goal is ultimately here to create an algorithm that will make predictions for new data values. This prediction will ultimately also be subject to potential artifacts, sampling bias, and noise. However, the target is creating an accurate prediction function and typically the error is measured by how close the predictions are to the truth. 

#### Internal "study design"

It is important to know your statistical analysis goal in advance. It has implications for most of the steps in your analysis. For example, with statistical inference you may choose more parsimonious models that are easier to understand and interpret; whereas for machine learning you may choose more sophisticated non-linear models if they improve prediction accuracy. 

One of the most important distinctions occurs right at the beginning of the analysis. If you are performing an inferential analysis you typically analyze the entire data set together at once, with the goal of making an estimate of uncertainty using the whole sample. When performing a statistical prediction or machine learning analysis you typically separate the data into training, testing, and validation sets so that you can build the statistical prediction in the training set, tune it in the testing set, and get an independent estimate of how well it works in the validation set. 

### Form an analysis plan

When you perform a statistical analysis you should start with a plan. This plan can be as simple as a list of steps and models you plan to perform or it can be as complicated as a complete set of code. But the important part is that you should _write your plan down in advance_. You can write down a very high level sketch of your analysis before you even begin exploration and a second, more thorough, analysis plan after you complete exploration. 
This is a particularly important step to complete if you have a complex, or high dimensional data set, if you have a vested interest or motivated collaborators who want the data to say something in particular, or if you are worried about over interpreting your data. The purpose of the analysis plan is to help you document all the post-hoc decisions that you made when analyzing your data. This documentation will allow both you and your collaborators or bosses to evaluate whether the decisions may lead to bias in your analysis. 

"Researcher degrees of freedom" is a term that was invented to refer to all the ways that you, as the analyst, can manipulate or change your analysis plan to try to reach a conclusion you already wanted in advance. The [title of their paper](https://journals.sagepub.com/doi/10.1177/0956797611417632) included the statement: 

> ...Undisclosed Flexibility in Data Collection and Analysis Allows Presenting Anything as Significant

They were specifically referring to statistical significance in the sense of identifying results with a P-value less than 0.05 as statistically significant. However, this same type of flexibility can lead to over-optimism in prediction, biased estimates, and generally incorrect analysis if they are not appropriately accounted for. So it is worth writing down an analysis plan you can compare to later when you set off to analyze any new data set. 

### Model signal

When you perform your exploratory analysis you will be looking for the "signal" in the data set. What is a signal? Typically we think of signal as the relationship between one or more variables. For example if you are looking for a relationship between x and y then the "signal" here is pretty obvious. 

```{r,echo=FALSE, warning=FALSE,messsage=FALSE}
library(tibble)
library(ggplot2)
library(dplyr)
set.seed(1234)
dat = tibble(x = rnorm(1000), z = rnorm(1000), y=x^3 + z)
dat %>% 
 ggplot(aes(x = x,y=y)) + 
        geom_point(color="grey") + 
        theme_minimal()
```


In fact, in this case the data are generated from the model:

$$ y  = x^3 + e$$ 

where $x \sim N(0,1)$ and $e \sim N(0,1)$. What we call the "signal" is the systematic relationship between $x$ and $y$ - so the $x^3$ part of the equation above. This represents the [typical](https://www.tandfonline.com/doi/full/10.1080/01621459.2020.1762613) relationship statisticians use to model data - they think of modeling a "surface" where the surface represents some simple function of the data with a noise term. In this case we might fit a model of the form: 

$$ y_i = f(x_i) + e_i$$ 

where $y_i$ is the $i$th data point, $f$ is a function relating $x$ to $y$ and $e_i$ represents unmodeled "noise" - which may be assumed to be random. In our simple example the function $f(x) = x^3$. When performing inference - or any statistical modeling - there is a tradeoff between simplifying interpretation and trying to capture the signal as precisely as possible. 

On the simple side of the scale, a default reaction for most data analysts is to start with a linear model. It is often a reasonable first summary of the data. 

```{r,warning=FALSE,messsage=FALSE}
library(modelr)
lm1 = lm(y ~ x, data=dat)
dat = dat %>%
        add_predictions(lm1)

dat %>% 
 ggplot(aes(x = x,y=y)) + 
        geom_point(color="grey") + 
        geom_line(aes(x=x,y=pred),color="black") + 
        theme_minimal()
```

Here this doesn't seem to capture the entire relationship between $x$ and $y$. But remember "all models are wrong...". We can _still think about the linear relationship between x and y even if it isn't the perfect model for the signal_. In particular, this model has the form: 

$$y_i = \beta_0 + \beta_1 x_i + e_i$$ 


Where $\beta_0$ is the average value of $y$ when $x = 0$ and $\beta_1$ is the average increase in $y$ for a one unit increase in $x$. In this example we get parameters estimates for each of the terms: 

```{r,warning=FALSE,messsage=FALSE}
library(broom)
lm1 %>% tidy(lm1)
```

So far so good. Remember, this line doesn't _perfectly_ represent the signal between $x$ and $y$. But it does represent our best estimate of the linear trend. Let's imagine that we could sample an infinite number of points. With infinite data you might get something that looks (approximately) like this: 

```{r,echo=FALSE,cache=TRUE,warning=FALSE,messsage=FALSE}
library(MASS)
set.seed(1234)
dat_big = tibble(x = rnorm(5e4), z = rnorm(5e4), y=x^3 + z)
dat_big %>% 
 ggplot(aes(x = x,y=y)) + 
        geom_tile()+ 
        stat_density_2d(aes(fill = stat(level)), geom = "polygon",show.legend=FALSE) +
        scale_fill_gradient(low = "white", high = "black") +
        theme_minimal() +
        xlim(-2,2) +
        ylim(-8,8)
```

This limiting case of infinite data is called the "super population". You can think of applying the same linear regression model to this infinite super population of data. If you do, the $\beta_1$ you get is the "parameter" you are estimating. The coefficient $\beta_1$ when fit to this infinite data is the exact value we are trying to estimate with our regression model. 

This seems pretty convoluted. In this case we can tell what the signal is exactly. So why think about the super population and define the parameter estimate as the "linear trend we would have observed in an infinite sample of data"? The reason is that while this case is simple and we know the true signal, we rarely will. So we are almost always using a summary of the data calculated with some simplified model. It is useful to think about what that model is trying to capture and what the result would be if we applied that summary to a data set where we could perfectly capture the same trend. 

The advantage of this approach is simple. If we are estimating the linear trend in this data, it does exist in the limit and when we get a parameter estimate we can interpret it easily: `r lm1$beta` is the average change in $y$ values for a one unit change in $x$ values. 

An alternative approach to capturing the "signal" is less focused on _attribution_ of the signal to a particular trend and more focused on capturing the most accurate representation we can with our simplified model. In that case we might fit a smooth function to the data.  There are a number of ways to [fit a smoother](https://rafalab.github.io/dsbook/smoothing.html) but one example is to fit a [generalized additive model](https://rafalab.github.io/pages/649/section-10.pdf). These models break what might be a complicated function of multiple variables:

$$ y = f(x_1,x_2,...,x_n) + e$$

and simplify them by assuming the terms are additive: 

$$ y = f(x_1) + f(x_2) + ...+ f(x_n) + e$$

Where the $f()$ functions can be as complicated or as simple as we like. We can fit this kind of model using the `gam` R package. 

```{r, warning=FALSE, message=FALSE}
library(mgcv)

gam1 = gam(y ~ s(x),data=dat)

dat %>% mutate(smooth = gam1$fitted) %>%
  ggplot(aes(x, y)) +
  geom_point(size = 3, alpha = .5, color = "grey") +
  geom_line(aes(x,smooth), color="red")
```

Here we "capture" the signal much better. But the resulting interpretation is a little bit harder. We have a smooth term (the $f()$ function), with an estimated number of degrees of freedom (a term describing how flexible the $f()$ function is). 

```{r}
gam1 %>% tidy()
```

This term doesn't have a neat interpretation of "a one unit change in x leads to a change of $\beta_1$ in y". Instead, we have to carefully describe the function and interpret what it means for the data. 

Typically, when modeling even complicated data sets, it makes sense to start out with the simple linear regression models. Fitting these models does not imply you think that the signal has a linear form - you are simply calculating a specific summary of the data. Then, if the simple linear models do not represent sufficiently useful summaries you can build more complicated models for the signal - carefully considering how you will interpret the resulting functions you estimate. 


### Account for artifacts

We typically think about the noise in a statistical model being random. However, they don't have to be! Let's take a really simple, totally deterministic example and show how the signals in the data can be due to unmeasured, systematic factors. This example was borrowed from [Ken Rice](http://faculty.washington.edu/kenrice/)'s linear models class. Imagine we have some [resistors](https://en.wikipedia.org/wiki/Resistor) that can be of one of two types - gold (whose resistance we denote $X$) and silver (whose resistance we denote $Z$). Our outcome is the total resistance $Y$.  

![](images/week8/resistors.png)

In this case everything is fully deterministic. If you show the resistance of gold versus the resistance of silver you see that they exactly add. 

![](images/week8/deterministic.png)


However, look what happens when we put them in different orders and take 


### Model uncertainty

### Compare to your analysis plan 

## Statistical modeling principles

### Look at your data

### Look at your models

### Simple to complex

### Understand uncertainty measures

### Hold yourself accountable


https://imgs.xkcd.com/comics/linear_regression_2x.png

### Document choices


## Additional Resources

:::resources

:::


## Homework 

:::homework
* __Template Repo__: https://github.com/advdatasci/homework8
* __Repo Name__: homework8-ind-yourgithubusername
* __Pull Date__: 2020/10/26 9:00AM Baltimore Time 
:::