[
["index.html", "Advanced Data Science 2020 1 Welcome and Syllabus 1.1 Assumptions and pre-requisites 1.2 Learning Objectives 1.3 Course Staff 1.4 Course logistics 1.5 Assignment Due Dates 1.6 The Pandemic 1.7 Grading 1.8 Assignments 1.9 Code of Conduct 1.10 Academic Ethics 1.11 Disability support services 1.12 Email alerts 1.13 Previous versions of the class 1.14 Typos and corrections", " Advanced Data Science 2020 Jeff Leek and Roger D. Peng 2020-09-14 1 Welcome and Syllabus Welcome! We are very excited to have you in our two-term (one semester) course on Advanced Data Science with course numbers 140.711 and 140.712 offered by the Department of Biostatistics at the Johns Hopkins Bloomberg School of Public Health. This course is designed for PhD students at Johns Hopkins Bloomberg School of Public Health. We are usually pretty flexible about permitting outside students but we want everyone to be aware of the goals and assumptions so no one feels like they are surprised by how the class works. The primary goal of the course is to teach you how to deconstruct, perform, and communicate professional data analyses across diverse media. The class is not designed to teach a set of statistical methods or packages - there are a ton of awesome classes, books, and tutorials about those things out there! Rather the goal is to help you to organize your thinking around how to combine the things you have learned about statistics, data manipulation, and visualization into complete data analyses that answer important questions about the world around you. 1.1 Assumptions and pre-requisites The course is designed for PhD students in the Johns Hopkins Biostatistics Masters and PhD programs and assumes significant background in statistics. Specifically we assume: You know the basics of statistics The central dogma (estimates, standard errors, basic distributions, etc.) Key statistical terms and methods Estimtion vs testing vs prediction You know how to fit and interpret statistical models Linear Models Generalized Linear Models Smoothing splines Basic mixture models You know the basics of R or Python You can read in, clean, tidy data You can fit models You can make visualizations You know the basics of reproducible research You know what version control is You know how to use Github You know how to use R/Rmarkdown Since the target audience for this course is advanced students in statistics we will not be able to spend significant time covering these concepts and technologies. To give you some idea about how these prerequisites will impact your experience in the course, we will be turning in all assignments via R markdown documents submitted through Github pull requests. The majority of the assignments will involve critiquing, fitting and interpreting statistical analyses - primarily focused on regression. Data analyses you will perform will also often involve significant data extraction, cleaning, and transformation. Hopefully all of that sounds familiar to you so you can focus on the concepts we will be teaching around deconstructing and constructing data analyses. Some resources that may be useful if you feel you may be missing pieces of this background: Statistics - Mathematical Biostatistics Bootcamp I (Coursera); Mathematical Biostatistics Bootcamp II (Coursera) Basic Data Science - Cloud Data Science (Leanpub); Data Science Specialization (Coursera) Version Control - Github Learning Lab; Happy Git and Github for the useR Rmarkdown - Rmarkdown introduction 1.2 Learning Objectives Our goal is by the end of our class: You will be able to critique a data analysis and separate good from bad analysis. Specifically you will be able to: Identify the underlying question Evaluate the “arc” of the data analysis Identify the underlying type of question Identify the study design Determine if visualizations are appropriate Determine if methods are appropriate Identify pipeline issues Identify reproducibility issues Identify common fallacies and mistakes Distinguish what is a real problem from what is just hard Identify common fallacies and mistakes. Evaluate the relationship between study design, data, and claims to data justification You will be able to produce a complete data analysis. Specifically you will learn to: Translate general questions to data analysis questions Explore your data skeptically Select appropriate data analytic tools given the study design Combine appropriate data analytic tools into pipelines Identify strengths and weaknesses of data pipelines you produce Describe the results of your analysis accurately Decide what is and is not relevant to the “arc” of the data analysis Write the “arc” of the data analysis Avoid “reinventing the wheel” You will be able to produce the components of a data analytic paper: The “arc” of a data analysis Abstracts Introductions Figures Tables Methods sections Discussion/limitations sections You will be able to produce the components of a methods paper: The “arc” of a methods paper Abstracts Introductions Figures Tables Simulation sections Applications sections Discussion/limitations sections You will be able to produce the components of a data analytic presentation for technical and non-technical audiences: Problem introduction Methods Results Conclusions You will be able to identify key issues in data analytic relationships. Specifically you will be able to: Elicit objective functions from collaborators Identify types of data analysis relationships (collaboration, consultation, employment) Identify successful stategies for data analysis based on relationship type Identify key ethical issues in data analysis Understand your responsibility as a data analyst Explain the value of data science to non-technical audiences 1.3 Course Staff The course instructors this year are Jeff Leek and Roger Peng. We are both professors in the Biostatistics Department at Johns Hopkins and Directors of the Johns Hopkins Data Science Lab. Jeff’s research focuses on human genomics, meta-research, and edtech for social good. Roger’s research focuses on air pollution, spatial statistics, and reproducibility. We have been friends for about 10 years and are excited to teach you some of the ins and outs of data science. We also have a couple of amazing TA’s this year: Eric Bridgeford who works on independence testing, manifold embedding, and graph inference; and Athena Chen work works on developing statistical tools to analyzing proteomic and genomic data to facilitate a deeper understanding of disease. 1.4 Course logistics This is a pretty unusual year because we will be entirely online. So our logistics will be a little different than usual. The course webpage will be here at: http://jtleek.com/ads2020/ All communication for the course is going to take place on one of four platforms: Slack - for discussion, sharing resources, collaborating, and announcements - Course slack channel: jhsph-ads-2020.slack.com Github - for submitting assignments - Course Github: https://github.com/jtleek/ads2020 Zoom - for live class discussions - Course Zoom: Link available on Course Slack Hypothesis for annotating/reviewing data analyses The primary communication for the class will go through Slack. That is where we will post course announcements, post all assignments, host most of our asynchronous course discussion, and as the primary means of communication between course participants and course instructors. You should request access to the JHU Advanced Data Science Course Slack immediately. The course TA’s will approve your access. Once you have access you will also be able to find the course Zoom and Zoom password. We will have two synchronous meetings a week for discussion (see section on Discussions below) - the class will be split into two approximately equal groups for these sessions: Available Times: - Mondays 9-10AM Baltimore Time - Mondays 1:30-2:30PM Baltimore Time Location: Zoom - link available on Slack For people who miss the sessions we will try to have a recap and notes that we will post to Slack so people can read them offline. If you haven’t already, please fill out the pre-course survey with your information and your preferred discussion time. 1.5 Assignment Due Dates All course assignment due dates will appear on the weekly course chapter. Please refer to these chapters for due dates. 1.6 The Pandemic This is how 2020 feels: It is super tough to be dealing with a pandemic, an economic crisis, challenges with visas and travel and coordinating school online. Your instructors understand that this is not an ordinary year. We are ultra sympathetic to family challenges and life challenges. We both have small children at home (who may make cameos in class discussions). Our goal is to make as much of the class asynchronous as possible so you can work whenever you have time, our plan is to be as understanding as possible when it comes to grading attendance, and any issues that come up with the course. Please don’t hesitate to reach out to us if you are having issues and we will do our best to direct you to whatever resources we have/accomodate you however we can. We think the material in this course is important, fun, and this is an opportunity to learn a lot. But life is more important than a course and if there was ever a year that life might get in the way of learning, this is that year. Good enough is the excellence of 2020. 1.7 Grading 1.7.1 Philosophy We believe the purpose of graduate education is to train you to be able to think for yourself and initiate and complete your own projects. We are super excited to talk to you about ideas, work out solutions with you, and help you to figure out how to produce professional data analyses. We don’t think that graduate school grades are important for this purpose. This means that we don’t care very much about graduate student grades. That being said, we have to give you a grade so they will be: A - Excellent - 90%+ B - Passing - 80%+ C - Needs improvement - 70%+ We rarely give out grades below a C and if you consistently submit work, participate in discussions, and do your best you are very likely to get an A or a B in the course. 1.7.2 Relative weights This course is primarily focused on deconstructing and constructing data analyses. The grading will be based on your participation in the course and helping each other improve your data analyses. The breakdown of grading will be: 40% for completing required reviews - see section on reviews below 40% for completing required data analysis assignments - see section on data analysis assignments below 20% for course participation on Slack and Zoom - see section on class participation below If you submit each review, it is your own work, and it meets a basic level of completeness and effort you will get 100% for that review. If you submit a review but it doesn’t meet basic completeness and effort you will receive 50%. If you do not submit a review you will receive 0%. If you submit a data analysis assignment, it is your own work, and it meets a basic level of completeness and effort you will get 100% for that data analysis assignment. If you submit a data analysis assignment but it doesn’t meet basic completeness and effort you will receive 50%. If you do not submit a review you will receive 0%. Grading participation is difficult in the best of circumstance and in a pandemic it is basically impossible. If you are at 80% of your assigned discussion sessions and participate in the discussion most of the time and respond to Slack prompts at least 5 times during the course of the term you will receive full participation points. If this level of course participation is challenging for you please reach out to the course instructors and we will work with you to figure out how to ensure you can participate sufficiently to get full points. 1.8 Assignments 1.8.1 Submitting assignments You will be invited to the JHSPH Advanced Data Science course organization: https://github.com/advdatasci. There will be one repo for each assignment. You will see two copies - the template repository (public) and your assigned homework repo which will be suffixed with your github user name (private). The assignment repos will include an Rmd file each week for you to fill out. For each assignment we will provide a time when we will pull your changes from Github. We will assume whatever version we pull at that time is what you are turning in. When we start peer reviewing each other’s work, after the submission deadline you will also be assigned another repo (private) with a suffix of your github user name and “peer-review”. This repo will include your peer’s work and a reviewing form which we will ask you to fill out and submit. After peer review is completed the feedback will be returned to you and you will be able to pull those changes to your computer, fill out a reviewer feedback form, and push your changes back to Github. Instruction submissions will be included with each assignment to remind you of the process you need to take and what dates/times to complete assignments. 1.8.2 Data Analysis Assignments After we have spent a few weeks reviewing data analytic work written by others, we will begin working on data analyses within 1.8.3 Data Analysis Reviews In this course we will be reviewing both published data analyses and each other’s work. You will review both in writing and orally during course discussions. Reviews in our course will take the following format: Written reviews For each assigned data analysis you will provide a written review which will include a summary of the data analysis and answers to key questions. At the beginning of the term these reviews will focus on published/public data analyses, but once we begin to turn in data analysis assignments, they will be peer reviews of each other’s work. Oral reviews The course will be broke up into two groups. Each group will meet once a week to discuss the papers/data analyses for that week. Each week, each group will have 2 discussion leaders: Lead Reviewer 1 and Lead Reviewer 2. We will rotate so each person gets to be a lead reviewer. Your responsibilities as lead reviewer are to: Lead Reviewer 1: Complete your written review and provide an overall summary of the data analysis and your answers to the questions. You are responsible for leading the discussion of the analysis. Lead Reviewer 2: Complete your written review and provide a second opinon on the data analysis, either supporting or providing new viewpoints of Reviewer 1. Both reviewers are encouraged to lead discussion of the data analysis to get feedback from the other participants in the session. 1.8.4 Reviewing Code of Conduct We will be reviewing both public work and each other’s work in both written and oral form. Reviewing well is an art form and is an important skill to master - and not just for this course! Regardless of where you go after Hopkins, you will be tasked with reviewing the work of others. The key principles of doing a good job in reviewing and the foundation for our course code of conduct are: Being concise - nothing extraneous Being precise - stating the specific problems with the manuscript or data analysis Being honest - stating any real issues you perceive Being constructive - stating how the authors could address the problems you have found Being polite - this helps focus on real issues rather than pet peeves. Reviewing each other’s work well is a critical challenge. Remember that there is a person behind the data analysis and you want them to improve. It is very easy to be sucked into the temptation to write a review that is entirely critical or even rude. The best reviews follow the guidelines above and are short, percise, documents that politely suggest constructive critiques. It takes practice to produce these kinds of reviews, which we will work on in class! One of the biggest privileges is the priveledge to say you don’t know or that you need something explained. I use this priveledge all the time and it makes it much easier for me when I’m trying to learn new concepts. I want you all to feel that privilege in this class. It is critical that we are able to have discussions in the class and everyone can voice their opinion without feeling looked down on. So let’s work together to allow everyone space to learn maximally. An amazing benefit of my privilege is being able to say “I didn't understand that. Could you explain it again?” as many times as necessary without having to worry that people will think I'm stupid. — Arvind Narayanan (@random_walker) August 26, 2020 Jeff has previously written a guide for written reviews of papers. 1.9 Code of Conduct We are committed to providing a welcoming, inclusive, and harassment-free experience for everyone, regardless of gender, gender identity and expression, age, sexual orientation, disability, physical appearance, body size, race, ethnicity, religion (or lack thereof), political beliefs/leanings, or technology choices. We do not tolerate harassment of course participants in any form. Sexual language and imagery is not appropriate for any work event, including group meetings, conferences, talks, parties, Twitter and other online media. This code of conduct applies to all course participants, including instructors and TAs, and applies to all modes of interaction, both in-person and online, including GitHub project repos, Slack channels, and Twitter. Course participants violating these rules will be referred to leadership of the Department of Biostatistics and the Title IX coordinator at JHU and may face expulsion from the class. All class participants agree to: Be considerate in speech and actions, and actively seek to acknowledge and respect the boundaries of other members. Be respectful. Disagreements happen, but do not require poor behavior or poor manners. Frustration is inevitable, but it should never turn into a personal attack. A community where people feel uncomfortable or threatened is not a productive one. Course participants should be respectful both of the other course participants and those outside the course. Refrain from demeaning, discriminatory, or harassing behavior and speech. Harassment includes, but is not limited to: deliberate intimidation; stalking; unwanted photography or recording; sustained or willful disruption of talks or other events; inappropriate physical contact; use of sexual or discriminatory imagery, comments, or jokes; and unwelcome sexual attention. If you feel that someone has harassed you or otherwise treated you inappropriately, please alert Jeff Leek or Roger Peng. Take care of each other. Refrain from advocating for, or encouraging, any of the above behavior. And, if someone asks you to stop, then stop. Alert Jeff Leek or Roger Peng if you notice a dangerous situation, someone in distress, or violations of this code of conduct, even if they seem inconsequential. 1.9.1 Need Help? Please speak with Jeff Leek or Roger Peng. You can also reach out to Karen Bandeen-Roche, chair of the department of Biostatistics or Margaret Taub, Ombudsman for the Department of Biostatistics. You may also reach out to any Hopkins resource for sexual harassment, discrimination, or misconduct: JHU Sexual Assault Helpline, 410-516-7333 (confidential) University Sexual Assault Response and Prevention website Johns Hopkins Compliance Hotline, 844-SPEAK2US (844-733-2528) Hopkins Policies Online JHU Office of Institutional Equity 410-516-8075 (nonconfidential) Johns Hopkins Student Assistance Program (JHSAP), 443-287-7000 University Health Services, 410-955-1892 The Faculty and Staff Assistance Program (FASAP), 443-997-7000 1.9.2 Feedback We welcome feedback on this Code of Conduct. 1.9.3 License and attribution This Code of Conduct is distributed under a CC-BY license. Portions of above text comprised of language from the Codes of Conduct adopted by rOpenSci and Django, which are licensed by CC BY-SA 4.0 and CC BY 3.0. This work was further inspired by Ada Initiative’s “how to design a code of conduct for your community” and Geek Feminism’s Code of conduct evaluations and expanded by Ashley Johnson and Shannon Ellis in the Leek group. 1.10 Academic Ethics Students enrolled in the Bloomberg School of Public Health of The Johns Hopkins University assume an obligation to conduct themselves in a manner appropriate to the University’s mission as an institution of higher education. A student is obligated to refrain from acts which he or she knows, or under the circumstances has reason to know, impair the academic integrity of the University. Violations of academic integrity include, but are not limited to: cheating; plagiarism; knowingly furnishing false information to any agent of the University for inclusion in the academic record; violation of the rights and welfare of animal or human subjects in research; and misconduct as a member of either School or University committees or recognized groups or organizations. Students should be familiar with the policies and procedures specified under Policy and Procedure Manual Student-01 (Academic Ethics), available on the school’s portal. The faculty, staff and students of the Bloomberg School of Public Health and the Johns Hopkins University have the shared responsibility to conduct themselves in a manner that upholds the law and respects the rights of others. Students enrolled in the School are subject to the Student Conduct Code (detailed in Policy and Procedure Manual Student-06) and assume an obligation to conduct themselves in a manner which upholds the law and respects the rights of others. They are responsible for maintaining the academic integrity of the institution and for preserving an environment conducive to the safe pursuit of the School’s educational, research, and professional practice missions. 1.11 Disability support services If you are a student with a documented disability who requires an academic accommodation, please contact the Office of Disability Support Services at 410-502-6602 or via email at JHSPH.dss@jhu.edu. Accommodations take effect upon approval and apply to the remainder of the time for which a student is registered and enrolled at the Bloomberg School of Public Health. 1.12 Email alerts The full course content will be available via this website. All assignments will be posted here and on Slack. But we are trying an experiment with Substack and you can sign up here for email alerts when new course chapters are available: https://jhuadvdatasci.substack.com/. If you aren’t at JHU but want to follow along with the content you are welcome to sign up as well! 1.13 Previous versions of the class https://jhu-advdatasci.github.io/2019/ https://jhu-advdatasci.github.io/2018/ http://jtleek.com/advdatasci/ http://jtleek.com/advdatasci16/ http://jtleek.com/advdatasci15/ https://github.com/jtleek/jhsph753and4 1.14 Typos and corrections Feel free to submit typos/errors/etc via the github repository associated with the class: https://github.com/jtleek/ads2020. You will have the thanks of your grateful instructors! "],
["week-1.html", "2 Week 1 2.1 Week 1 Learning objectives 2.2 What is advanced data science anyway? 2.3 Types of data analytic questions 2.4 A data analytic rubric 2.5 Your first assignment - deconstructing an analysis 2.6 Additional Resources 2.7 Homework", " 2 Week 1 2.1 Week 1 Learning objectives At the end of this lesson you will: Be able to define data science and advanced data science Be able to define the types of data analytic questions Be able to follow a data analysis rubric to evaluate an analysis 2.2 What is advanced data science anyway? 2.2.1 Maybe we should start by defining data science…. Before we can define advanced data science we need to define data science. The definition we will use is: Data science is the process of formulating a quantitative question that can be answered with data, collecting and cleaning the data, analyzing the data, and communicating the answer to the question to a relevant audience. In general the data science process is iterative and the different components blend together a little bit. But for simplicity lets discretize the tasks into the following 7 steps: Define the question of interest Get the data Clean the data Explore the data Fit statistical models Communicate the results Make your analysis reproducible The reality is that the process is usually much more iterative. This is an excellent diagram describing the usual flow of a data science project by a former student in this class, Simina Boca: Feeling preeety good about this diagram that I wrote in sparkly pens for the data analysis class I'm teaching, which starts tomorrow… Hope it's clear now that data scientists and applied statisticians don't simply press a 🖱️or wave a 🪄! Feedback welcome for future iterations! pic.twitter.com/BoDeyUuNvT — Simina M. Boca (@siminaboca) August 27, 2020 A good data science project answers a real scientific or business analytics question. In almost all of these experiments the vast majority of the analyst’s time is spent on getting and cleaning the data (steps 2-3) and communication and reproducibility (6-7). In most cases, if the data scientist has done her job right the statistical models don’t need to be incredibly complicated to identify the important relationships the project is trying to find. In fact, if a complicated statistical model seems necessary, it often means that you don’t have the right data to answer the question you really want to answer. As Tukey said: The combination of some data and an aching desire for an answer does not ensure that a reasonable answer can be extracted from a given body of data. One option is to spend a huge amount of time trying to tune a statistical model to try to answer the question but serious data scientist’s usually instead try to go back and get the right data. The result of this process is that most well executed and successful data science projects don’t (a) use super complicated tools or (b) fit super complicated statistical models. The characteristics of the most successful data science projects I’ve evaluated or been a part of are: (a) a laser focus on solving the scientific problem, (b) careful and thoughtful consideration of whether the data is the right data and whether there are any lurking confounders or biases and (c) relatively simple statistical models applied and interpreted skeptically. 2.2.2 Data Science is hard, but not like math is hard It turns out doing those three things is actually surprisingly hard and very, very time consuming. It is my experience that data science projects take a solid 2-3 times as long to complete as a project in theoretical statistics. The reason is that inevitably the data are a mess and you have to clean them up, then you find out the data aren’t quite what you wanted to answer the question, so you go find a new data set and clean it up, etc. After a ton of work like that, you have a nice set of data to which you fit simple statistical models and then it looks super easy to someone who either doesn’t know about the data collection and cleaning process or doesn’t care. This poses a major public relations problem for serious data scientists. When you show someone a good data science project they almost invariably think “oh that is easy” or “that is just a trivial statistical/machine learning model” and don’t see all of the work that goes into solving the real problems in data science. A concrete example of this is in academic statistics. It is customary for people to show theorems in their talks and maybe even some of the proof. This gives people working on theoretical projects an opportunity to “show their stuff” and demonstrate how good they are. The equivalent for a data scientist would be showing how they found and cleaned multiple data sets, merged them together, checked for biases, and arrived at a simplified data set. Showing the “proof” would be equivalent to showing how they matched IDs. These things often don’t look nearly as impressive in talks, particularly if the audience doesn’t have experience with how incredibly delicate real data analysis is. I imagine versions of this problem play out in industry as well (candidate X did a good analysis but it wasn’t anything special, candidate Y used Hadoop to do BIG DATA!). The really tricky twist is that bad data science looks easy too. You can scrape a data set off the web and slap a machine learning algorithm on it no problem. So how do you judge whether a data science project is really “hard” and whether the data scientist is an expert? Just like with anything, there is no easy shortcut to evaluating data science projects. You have to ask questions about the details of how the data were collected, what kind of biases might exist, why they picked one data set over another, etc. In the meantime, don’t be fooled by what looks like simple data science - it can often be pretty effective. This course is designed for PhD students in Biostatistics and most of the courses you have taken have been hard by virtue of mathematical difficulty. These courses focus on deductive resasoning whereby you are told a set of principles or facts and you deduce logically some conclusions through proofs. The steps may be tricky and may require deep mathematical understanding. But the important point is that there is a right answer to most of these problems. Data science is much more akin to inductive reasoning. Inductive reasoning involves taking a small set of representative examples (say a sample of data) and trying to generalize these examples to make a broader statement (say a population). Even when the data are correct, the conculions you may draw can be wildly inaccurate. So the hard thing about data science is describing a path from a set of known data to a set of conclusions that can be supported by the data. Ideally, these conclusions will hold up to scrutiny, skepticism, and replication. In other words, data science is hard precisely because there is often not a “right” answer. Good data science is distinguished from bad data science primarily by a repeatable, thoughtful, skeptical application of an analytic process to data in order to arrive at supportable conclusions. Andrew Gelman Paper on Inductive vs Deductive Reasoning 2.2.3 So what is advanced data science? Ask yourselves, what problem have you solved, ever, that was worth solving, where you knew knew all of the given information in advance? Where you didn’t have a surplus of information and have to filter it out, or you didn’t have insufficient information and have to go find some? This quote comes from a Dan Meyer Ted Talk about patient problem solving. But it applies equally to data science. Data science is answering questions with data and it requires a range of skills and therefore a range of classes: Level 0: Background: Basic computing, some calculus with a focus on optimization, basic linear algebra. Level 1: Data science thinking: How to define a question, how to turn a question into a statement about data, how to identify data sets that may be applicable, experimental design, critical thinking about data sets. Level 2: Data science communication: Teaching students how to write about data science, how to express models qualitatively and in mathematical notation, explaining how to interpret results of algorithms/models. Explaining how to make figures. Level 3: Data science tools: Learning the basic tools of R, loading data of various types, reading data, plotting data. Level 4: Real data: Manipulating different file formats, working with “messy” data, trying to organize multiple data sets into one data set. Level 5: Worked examples: Use real data examples, but work them through from start to finish as case studies, don’t make them easy clean data sets, but have a clear path from the beginning of the problem to the end. Level 6: Just the question: Give students a question where you have done a little research to know that it is posisble to get at least some data, but aren’t 100% sure it is the right data or that the problem can be perfectly solved. Part of the learning process here is knowing how to define success or failure and when to keep going or when to quit. Level 7: The student is the scientist: Have the students come up with their own questions and answer them using data. As you move up the hierarchy of data science classes, the emphasis moves away from technological skills and toward synthesis and communication. The hardest part of data science isn't the technology pic.twitter.com/2IslFWrJwa — Caitlin Hudon 👩🏼‍💻 (@beeonaposy) August 26, 2020 This advanced data science course assumes you have background in statistics, programming, and the basics of project management - it will instead focus on synthesizing these tools into a data analysis and communicating the analysis to an audience. We will instead focus on the “hard” part of data science, which is understanding the way to use data to make generalizable statements about the world and communicating those results to others. Data analysis is often (incorrectly) distilled down to the set of claims or a whether a p-value is lower than some threshold. But the tip of this iceberg conceals a large number of analytic choices, human behaviors, biases, and conventions that underly the data analytic process. This iceberg inspired the ADS 2020 course hex sticker. In this course we will focus on the parts of the data analysis that are often overlooked, but are critical to data analytic success. 2.3 Types of data analytic questions Data can be used to answer many questions, but not all of them. One of the most innovative data scientists of all time said it best. The data may not contain the answer. The combination of some data and an aching desire for an answer does not ensure that a reasonable answer can be extracted from a given body of data. John Tukey Before performing a data analysis the key is to define the type of question being asked. Some questions are easier to answer with data and some are harder. This is a broad categorization of the types of data analysis questions, ranked by how easy it is to answer the question with data. You can also use the data analysis question type flow chart to help define the question type (from the paper What is the question ) The data analysis question type flow chart 2.3.1 Descriptive A descriptive data analysis seeks to summarize the measurements in a single data set without further interpretation. An example is the United States Census. The Census collects data on the residence type, location, age, sex, and race of all people in the United States at a fixed time. The Census is descriptive because the goal is to summarize the measurements in this fixed data set into population counts and describe how many people live in different parts of the United States. The interpretation and use of these counts is left to Congress and the public, but is not part of the data analysis. 2.3.2 Exploratory An exploratory data analysis builds on a descriptive analysis by searching for discoveries, trends, correlations, or relationships between the measurements of multiple variables to generate ideas or hypotheses. An example is the discovery of a four-planet solar system by amateur astronomers using public astronomical data from the Kepler telescope. The data was made available through the planethunters.org website, that asked amateur astronomers to look for a characteristic pattern of light indicating potential planets. An exploratory analysis like this one seeks to make discoveries, but rarely can confirm those discoveries. In the case of the amateur astronomers, follow-up studies and additional data were needed to confirm the existence of the four-planet system. 2.3.3 Inferential An inferential data analysis goes beyond an exploratory analysis by quantifying whether an observed pattern will likely hold beyond the data set in hand. Inferential data analyses are the most common statistical analysis in the formal scientific literature. An example is a study of whether air pollution correlates with life expectancy at the state level in the United States. The goal is to identify the strength of the relationship in both the specific data set and to determine whether that relationship will hold in future data. In non-randomized experiments, it is usually only possible to observe whether a relationship between two measurements exists. It is often impossible to determine how or why the relationship exists - it could be due to unmeasured data, relationships, or incomplete modeling. 2.3.4 Predictive While an inferential data analysis quantifies the relationships among measurements at population-scale, a predictive data analysis uses a subset of measurements (the features) to predict another measurement (the outcome) on a single person or unit. An example is when organizations like FiveThirtyEight.com use polling data to predict how people will vote on election day. In some cases, the set of measurements used to predict the outcome will be intuitive. There is an obvious reason why polling data may be useful for predicting voting behavior. But predictive data analyses only show that you can predict one measurement from another, they don’t necessarily explain why that choice of prediction works. 2.3.5 Causal A causal data analysis seeks to find out what happens to one measurement if you make another measurement change. An example is a randomized clinical trial to identify whether fecal transplants reduces infections due to Clostridium dificile. In this study, patients were randomized to receive a fecal transplant plus standard care or simply standard care. In the resulting data, the researchers identified a relationship between transplants and infection outcomes. The researchers were able to determine that fecal transplants caused a reduction in infection outcomes. Unlike a predictive or inferential data analysis, a causal data analysis identifies both the magnitude and direction of relationships between variables. 2.3.6 Mechanistic Causal data analyses seek to identify average effects between often noisy variables. For example, decades of data show a clear causal relationship between smoking and cancer. If you smoke, it is a sure thing that your risk of cancer will increase. But it is not a sure thing that you will get cancer. The causal effect is real, but it is an effect on your average risk. A mechanistic data analysis seeks to demonstrate that changing one measurement always and exclusively leads to a specific, deterministic behavior in another. The goal is to not only understand that there is an effect, but how that effect operates. An example of a mechanistic analysis is analyzing data on how wing design changes air flow over a wing, leading to decreased drag. Outside of engineering, mechanistic data analysis is extremely challenging and rarely undertaken. 2.4 A data analytic rubric There is often no “right” or “wrong” answer when evaluating a data analysis, but there are some characteristics that separate good analyses from poor analyses. One difficult thing about advanced data science is that these rules have not been formally defined like much of our statistical theory and are not generally agreed on. We are only at the beginning of developing a “theory” of data analysis - we will learn more about these efforts later in the class. You can think of the theory of data analysis more like guidance than hard and fast rules. Roger Peng outlines this idea in his Dean’s lecture which I would encourage you to watch (the key analogy to music begins around 27 minutes in). For now we will use a basic checklist (adapted from the book Elements of Data Analytic Style) when reviewing data analyses. It can be used as a guide during the process of a data analysis, as a rubric for grading data analysis projects, or as a way to evaluate the quality of a reported data analysis. You don’t have to answer every one of these questions for every data analysis, but they are a useful set of ideas ot keep in the back of your mind when reviewing a data analysis. 2.4.1 Answering the question Did you specify the type of data analytic question (e.g. exploration, association causality) before touching the data? Did you define the metric for success before beginning? Did you understand the context for the question and the scientific or business application? Did you record the experimental design? Did you consider whether the question could be answered with the available data? 2.4.2 Checking the data Did you plot univariate and multivariate summaries of the data? Did you check for outliers? Did you identify the missing data code? 2.4.3 Tidying the data Is each variable one column? Is each observation one row? Do different data types appear in each table? Did you record the recipe for moving from raw to tidy data? Did you create a code book? Did you record all parameters, units, and functions applied to the data? 2.4.4 Exploratory analysis Did you identify missing values? Did you make univariate plots (histograms, density plots, boxplots)? Did you consider correlations between variables (scatterplots)? Did you check the units of all data points to make sure they are in the right range? Did you try to identify any errors or miscoding of variables? Did you consider plotting on a log scale? Would a scatterplot be more informative? 2.4.5 Inference Did you identify what large population you are trying to describe? Did you clearly identify the quantities of interest in your model? Did you consider potential confounders? Did you identify and model potential sources of correlation such as measurements over time or space? Did you calculate a measure of uncertainty for each estimate on the scientific scale? 2.4.6 Prediction Did you identify in advance your error measure? Did you immediately split your data into training and validation? Did you use cross validation, resampling, or bootstrapping only on the training data? Did you create features using only the training data? Did you estimate parameters only on the training data? Did you fix all features, parameters, and models before applying to the validation data? Did you apply only one final model to the validation data and report the error rate? 2.4.7 Causality Did you identify whether your study was randomized? Did you identify potential reasons that causality may not be appropriate such as confounders, missing data, non-ignorable dropout, or unblinded experiments? If not, did you avoid using language that would imply cause and effect? 2.4.8 Written analyses Did you describe the question of interest? Did you describe the data set, experimental design, and question you are answering? Did you specify the type of data analytic question you are answering? Did you specify in clear notation the exact model you are fitting? Did you explain on the scale of interest what each estimate and measure of uncertainty means? Did you report a measure of uncertainty for each estimate on the scientific scale? 2.4.9 Figures Does each figure communicate an important piece of information or address a question of interest? Do all your figures include plain language axis labels? Is the font size large enough to read? Does every figure have a detailed caption that explains all axes, legends, and trends in the figure? 2.4.10 Presentations Did you lead with a brief, understandable to everyone statement of your problem? Did you explain the data, measurement technology, and experimental design before you explained your model? Did you explain the features you will use to model data before you explain the model? Did you make sure all legends and axes were legible from the back of the room? 2.4.11 Reproducibility Did you avoid doing calculations manually? Did you create a script that reproduces all your analyses? Did you save the raw and processed versions of your data? Did you record all versions of the software you used to process the data? Did you try to have someone else run your analysis code to confirm they got the same answers? 2.4.12 R packages Did you make your package name “Googleable” Did you write unit tests for your functions? Did you write help files for all functions? Did you write a vignette? Did you try to reduce dependencies to actively maintained packages? Have you eliminated all errors and warnings from R CMD CHECK? 2.5 Your first assignment - deconstructing an analysis Before we leap into doing data analysis we are going to deconstruct some data analyses to help us discover what are the parts that work and don’t in different contexts. Your first assignment will be to read this data analysis about Mortality in the aftermath of Hurricane Maria. You will answer a series of questions, then we will discuss the analysis during class. Write a brief one paragraph summary of the paper highlighting what you consider to be the key parts of the analysis. Who you think the target audience of this paper is? What kind of question (descriptive, exploratory, inferenential, predictive, casual, or mechanistic) is this paper trying to answer? What is the main question the paper is trying to answer? Do you think the data used in the paper are sufficient to answer the question? What are the main sections of the paper? How do they support (or not) the answer to the question? Do you think the analytic methods (plots, summaries, statistical models) are sufficient to answer the question? Do you think it could have been done with simpler methods? Do you think you could reproduce this analysis based on the text? Do you have access to the data, the code, enough of a description of what happened? Why or why not? Do you think that the analysis was done in the order shown in the paper? Do you think the authors only did the analysis shown in the paper? Do you think that the overall analysis makes a convincing point? Why or why not? 2.6 Additional Resources The Elements of Data Analytic Style {data analysis book} The Art of Data Science {data analysis book} Advanced Data Analysis from an Elementary Point of View {methods book} An Introduction to Statistical Learning {methods book} 2.7 Homework Template Repo: https://github.com/advdatasci/homework1 Repo Name: homework1-ind-yourgithubusername Pull Date: 2020/09/07 9:00AM Baltimore Time "],
["week-2.html", "3 Week 2 3.1 Week 2 Learning objectives 3.2 The steps in a data analysis 3.3 Raw, informal, and formal data science 3.4 Additional Resources 3.5 Homework", " 3 Week 2 3.1 Week 2 Learning objectives At the end of this lesson you will be able to: Define the steps in a data analysis Structure a data science project Identify the difference between the levels of an analysis - Raw - Informal - Formal Identify the steps between raw, informal and formal analysis 3.2 The steps in a data analysis Recall from week 1 the definition of data science: Data science is the process of formulating a quantitative question that can be answered with data, collecting and cleaning the data, analyzing the data, and communicating the answer to the question to a relevant audience. When people talk about data science they often think of big data, or complicated statistical machine learning tools. But the key word in data science is not data, it is science. Specifically the fastest way to end up with an unsuccessful data science project is to start with the data or tools. Good data science almost invariably starts with a good question. Most data science projects follow a common set of steps: Define the question Define the ideal data set Determine what data you can access Obtain the data Clean the data Exploratory data analysis Statistical prediction/modeling Interpret results Challenge results Synthesize/write up results Create reproducible code These steps might vary somewhat depending on the context of the project, but are remarkably consistent across projects in both industry and academia. To illustrate this process I’m going to use a simple, somewhat contrived, example - but it can equally be applied to any new data science project you start. 3.2.1 Defining the question The difference between real success and failure in data analysis often comes down to the question. The key to settling on a good question is to first start without thinking about the data that you have. Think only about the question you care about. It has been suggested that data makes the scientific method obsolete - but the reality is that big data makes careful scientific thinking more important, not less. Regardless of who is involved in the question definition process there are four criteria that help define a good data science question. It should be specific It should be answerable with data It should be answerable with the data you have It should not be designed to discriminate or cause harm It should come with a definition of success So we might start with a general question - something like: Is it possible to consistently label hand-drawn images of hardware using a simple model? This question could be of interest if, for example, you are building a hand-drawn search feature for a hardware store app or website. Note that this question was defined without thinking about what data we might collect, what data we have on hand, or really any of the potential constraints. It is generally a good idea to start with the question you really care about - ignoring these constraints - and work down to a more specific, answerable question. 3.2.2 Defining the ideal data set The ideal data set might be a large collection of hand drawn images, labeled with the intention of each of the people who made the drawing. This kind of data would allow you to directly develop a classification algorithm and measure its success. However, this might also be an expensive data set to collect. You would need to pay people to make the drawings and take the time to go through your inventory with them to get them to provide gold standard labels. This might be well beyond the resources you have available to you - so you might have to settle for a less than ideal data set. However, it is always important to consider what ideal looks like. At a minimum, this will allow you to identify specific, concrete potential limitations of the data you end up using. 3.2.3 Determine the data you can access If you hadn’t already collected data to answer this question, you might go looking for convenience data. For example you might land on the Google Quickdraw project. The QuickDraw project asks people to draw pictures of different objects and then uses a neural network to predict what has been drawn. The data consist of a large number of hand drawn images and the neural-network predicted labels for those objects from Google. So in applying the criteria above we need to refine our question to be more specific and answerable with the data we have in hand: Can we classify images based on the data collected by QuickDraw into labeled categories from the neural network? Sometimes this would be where the data science project may stop. The question may either not be answerable with data at all - or we may not have sufficiently good data to answer the question. Expert data scientists learn through years of practice to identify questions that are likely to lead to fruitful conclusions. For example, in this case past experience has shown that classification of images with high-accuracy is quite possible - at least with complicated models.. While this data is large and potentially incredibly useful for the question we originally set out to answer, it also has some limitations. Since we defined the ideal data set to start, we can use that as a way to highlight potential limitations of our analysis. For example: The data were collected on the internet at large, and not necessarily from the potential customers at the hardware store. The labels are not defined by the the artist, but by the neural network from Google and so may be inaccurate. The pictures represent a large collection of objects, not simply hardware, and so may not reflect all of the types of images you may see in practice. Another important question is whether the proposed application is ethical or a good idea. There are entire courses on the ethics of data science and we will cover this in more detail later in the course. But for this simple example - we might also want to consider relevant questions such as: How will this classifier work for people with disabilities? Are there cultural differences that will make the classifier work better for certain groups of people? What should happen if a person draws something inappropriate? Despite these limitations and considerations, this data is public, free, and large, so represents an opportunity to start to answer our question. Here we will make the question even more specific, simply for ease of exposition. We will narrow the set of data to the hand drawn images of axes and clouds and see if we can separate the two. The final step in defining the question is coming up with the definition of success. It is critically important to do this before you start and will depend heavily on the context. For example, in our simple case we might only need classification accuracy of 80% for the project to be successful - 4 out of 5 customers will be directed to the right project and the other won’t be harmed. But if you are building a self-driving car, you might need to be significantly more accurate! However, using this definition of success we can write down our final question: Can we classify images of clouds and axes to 80% accuracy based on the data collected by QuickDraw into labeled categories from the neural network? This question only answers a fraction of our initial, broad question. That is often the case! Most data science problems start out with a major open question and narrow down to a question answerable with a given data set. One important consideration we have not discussed here is that as a data scientist you are often not the only person involved in the definition of the question. It may be driven by a scientific collaborator, a project manager, or an executive in industry. The process should still be the same - moving from an important content driven question and drilling down to a specific, answerable data science question. It is generally helpful in these conversations to be open about the entire process of question definition - from generality to specificity. This process may involve significant iteration and will require good communication between you and your collaborators or managers. Often the most difficult part of this process is managing expectations - people without significant data analysis expertise will often expect the data to be capable of answering more, or harder, questions than they actually can. 3.2.4 Obtain the data Now that we have defined our question, we need to obtain some data. In this case, the QuickDraw data are publicly available. You can download them from the web. This process also might involve pulling data from an API, accessing a database and sampling data, or collecting the data directly. Having a variety of tools at your disposal for collecting (sometimes called pulling) data is an important component of your toolbox as a data scientist. There are a number of courses and resources focused on getting data including: Getting and cleaning data Getting data Databases using dplyr Data scraping in R In our simple example, I have downloaded and subsampled the data to make them usable for this analysis. One important thing I haven’t done - but is a good idea in general - is to write code that will automatically and reproducibly pull the data from the source. Including this step as a component of your data analytic code will save you a lot of time in the future when a collaborator asks you to update your analysis with the latest data. We will discuss this more when we talk about structuring data science projects. 3.2.5 Clean the data It has been said that data science is 80% data cleaning and that continues to hold true (at least if the data scientists on Twitter are to be trusted): Have been extremely curious about this for a while now, so I decided to create a poll. “As someone titled ‘data scientist’ in 2019, I spend most of (60%+) my time:” (“Other”) also welcome, add it in the replies. — Vicki Boykis (@vboykis) January 28, 2019 Data cleaning is one of the hardest things to teach. The reason is best summed up in this quote by Hadley Wickham with apologies to Tolstoy: tidy datasets are all alike but every messy dataset is messy in its own way. Each type of data you encounter will have different steps involved in the process. But data cleaning always involves identifying and cataloging data quirks, iconsistencies, and errors and reformatting or reshaping data to the form needed for analysis. In our simple example we can find the data here: https://github.com/jtleek/ads2020/tree/master/docs/resources/week2/. In cases where the data are large or unwieldy an important principle is to make the data small enough to work with. Here we do that by random sampling: Robert Gentleman, Genentech: “make big data as small as possible as quick as is possible” to enable sharing #bigdatamed — Ellie McDonagh (@EllieMcDonagh) May 21, 2014 library(LaF) library(here) axes_json = sample_lines(here::here(&quot;docs/resources/week2/axes.ndjson&quot;),100) clouds_json = sample_lines(here::here(&quot;docs/resources/week2/clouds.ndjson&quot;),100) axes_json[[1]] ## [1] &quot;{\\&quot;word\\&quot;:\\&quot;axe\\&quot;,\\&quot;countrycode\\&quot;:\\&quot;US\\&quot;,\\&quot;timestamp\\&quot;:\\&quot;2017-01-28 03:42:15.54694 UTC\\&quot;,\\&quot;recognized\\&quot;:true,\\&quot;key_id\\&quot;:\\&quot;6620339809288192\\&quot;,\\&quot;drawing\\&quot;:[[[92,82,62,2,0,23,30,31,28,41,79,112],[30,44,54,55,59,92,112,146,156,152,132,129]],[[95,97,107,114,114,124,164,164,153,151,130],[34,76,104,166,192,255,244,216,154,81,0]]]}&quot; The next thing I did was google “quick draw data ndjson rstats”. I found a tutorial and lifted some code for processing ndjson data into data frames. This is a common step in data cleaning - since it is so bespoke by data type you will often want to use Google to search for the data type and the type of cleaning you want to do. library(dplyr) parse_drawing = function(list) { lapply(list$drawing, function(z) {data_frame(x=z[[1]], y=z[[2]])}) %&gt;% bind_rows(.id = &quot;line&quot;) %&gt;% mutate(drawing=list$key_id, row_id=row_number()) } first_axe = rjson::fromJSON(axes_json[[1]]) %&gt;% parse_drawing() first_axe ## # A tibble: 23 x 5 ## line x y drawing row_id ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;int&gt; ## 1 1 92 30 6620339809288192 1 ## 2 1 82 44 6620339809288192 2 ## 3 1 62 54 6620339809288192 3 ## 4 1 2 55 6620339809288192 4 ## 5 1 0 59 6620339809288192 5 ## 6 1 23 92 6620339809288192 6 ## 7 1 30 112 6620339809288192 7 ## 8 1 31 146 6620339809288192 8 ## 9 1 28 156 6620339809288192 9 ## 10 1 41 152 6620339809288192 10 ## # … with 13 more rows 3.2.6 Exploratory data analysis Now that we have cleaned the data up into a format we can use we can start to explore the data. For most data analysis projects, exploratory data analysis and data cleaning will go hand-in-hand and it will often be iterative between the two steps. The key point is that you should use EDA to discover issues with the data, sources of variation, and key factors you must account for in downstream analysis. The process is very similar to the parable of the blind men and the elephant: A group of blind men heard that a strange animal, called an elephant, had been brought to the town, but none of them were aware of its shape and form. Out of curiosity, they said: “We must inspect and know it by touch, of which we are capable”. So, they sought it out, and when they found it they groped about it. The first person, whose hand landed on the trunk, said, “This being is like a thick snake”. For another one whose hand reached its ear, it seemed like a kind of fan. As for another person, whose hand was upon its leg, said, the elephant is a pillar like a tree-trunk. The blind man who placed his hand upon its side said the elephant, “is a wall”. Another who felt its tail, described it as a rope. The last felt its tusk, stating the elephant is that which is hard, smooth and like a spear. Similarly, when doing your initial data cleaning you will be looking at different parts of the data and trying to get a mental picture of the complete data set before moving on to modeling. Here we can do some simple EDA and look at a plot of one of the axes library(ggplot2) ggplot(first_axe,aes(x, y)) + geom_point() + scale_x_continuous(limits=c(0, 255))+ scale_y_reverse(limits=c(255, 0))+ theme_minimal() then we might fill in the lines to get a better idea of what the drawing looks like to the customer: ggplot(first_axe,aes(x, y)) + geom_path(aes(group = line), lwd=1)+ scale_x_continuous(limits=c(0, 255))+ scale_y_reverse(limits=c(255, 0))+ theme_minimal() then we might look at a few more axes to get a handle on the variation in the data set: rjson::fromJSON(axes_json[[2]]) %&gt;% parse_drawing() %&gt;% ggplot(aes(x, y)) + geom_path(aes(group = line), lwd=1)+ scale_x_continuous(limits=c(0, 255))+ scale_y_reverse(limits=c(255, 0))+ theme_minimal() then compare these drawings to drawings of clouds rjson::fromJSON(clouds_json[[1]]) %&gt;% parse_drawing() %&gt;% ggplot(aes(x, y)) + geom_path(aes(group = line), lwd=1)+ scale_x_continuous(limits=c(0, 255))+ scale_y_reverse(limits=c(255, 0))+ theme_minimal() A bunch of data processing has been done for us, but the data aren’t quite ready to be fed into an algorithm yet. To do that, we’d need a data frame with each row equal to one drawing and each column equal to one feature for that drawing, with an extra column for the drawing output. Can we think of how we’d do that for a data set like this? Here are two things I thought of: I need the points sampled on a regular grid I need the points to be of a manageable size Note that these are semi-arbitrary choices. A lot of data processing is based on the intuition/best guess of the analyst. While this shouldn’t be a cause for alarm, it is important that you document all of these choices as they may have an impact on the answers you ultimately obtain; these choices may need to be explored and explained when interpreting your results. # Let’s start by creating a regular grid of 256 x and y values. library(tibble) grid_dat = as.tibble(expand.grid(x = 1:256,y=1:256)) # Now we could make each x,y value be a grid point with a join - this is overkill grid_axe = left_join(grid_dat,first_axe) # Let’s add an indicator of whether a particular value is zero or not. grid_axe = grid_axe %&gt;% mutate(pixel = ifelse(is.na(line),0,1)) # subsample to a smaller size library(Hmisc) grid_axe$xgroup = cut2(grid_axe$x,g=16,levels.mean=TRUE) grid_axe$ygroup = cut2(grid_axe$y,g=16,levels.mean=TRUE) grid_axe ## # A tibble: 65,536 x 8 ## x y line drawing row_id pixel xgroup ygroup ## &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;int&gt; &lt;dbl&gt; &lt;fct&gt; &lt;fct&gt; ## 1 1 1 &lt;NA&gt; &lt;NA&gt; NA 0 &quot; 8.5&quot; &quot; 8.5&quot; ## 2 2 1 &lt;NA&gt; &lt;NA&gt; NA 0 &quot; 8.5&quot; &quot; 8.5&quot; ## 3 3 1 &lt;NA&gt; &lt;NA&gt; NA 0 &quot; 8.5&quot; &quot; 8.5&quot; ## 4 4 1 &lt;NA&gt; &lt;NA&gt; NA 0 &quot; 8.5&quot; &quot; 8.5&quot; ## 5 5 1 &lt;NA&gt; &lt;NA&gt; NA 0 &quot; 8.5&quot; &quot; 8.5&quot; ## 6 6 1 &lt;NA&gt; &lt;NA&gt; NA 0 &quot; 8.5&quot; &quot; 8.5&quot; ## 7 7 1 &lt;NA&gt; &lt;NA&gt; NA 0 &quot; 8.5&quot; &quot; 8.5&quot; ## 8 8 1 &lt;NA&gt; &lt;NA&gt; NA 0 &quot; 8.5&quot; &quot; 8.5&quot; ## 9 9 1 &lt;NA&gt; &lt;NA&gt; NA 0 &quot; 8.5&quot; &quot; 8.5&quot; ## 10 10 1 &lt;NA&gt; &lt;NA&gt; NA 0 &quot; 8.5&quot; &quot; 8.5&quot; ## # … with 65,526 more rows # Now I can convert these to numbers so we’ll have them later grid_axe = grid_axe %&gt;% mutate(xgroup = as.numeric(as.character(xgroup)) - 7.5) %&gt;% mutate(ygroup = as.numeric(as.character(ygroup)) - 7.5) # Then average within groups of pixels to get a smaller value small_axe = grid_axe %&gt;% group_by(xgroup,ygroup) %&gt;% summarise(pixel=mean(pixel)) small_axe ## # A tibble: 256 x 3 ## # Groups: xgroup [16] ## xgroup ygroup pixel ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1 1 0 ## 2 1 17 0 ## 3 1 33 0 ## 4 1 49 0.00391 ## 5 1 65 0 ## 6 1 81 0 ## 7 1 97 0 ## 8 1 113 0 ## 9 1 129 0 ## 10 1 145 0 ## # … with 246 more rows Remember this was our original axe ggplot(first_axe,aes(x, y)) + geom_point() + scale_x_continuous(limits=c(0, 255))+ scale_y_reverse(limits=c(255, 0))+ theme_minimal() Now we can look at the small version - after cleaning, sampling to a grid, downsampling to shrink the data and averaging pixels - it looks similar - whew! :) ggplot(small_axe %&gt;% filter(pixel &gt; 0),aes(xgroup, ygroup)) + geom_point() + scale_x_continuous(limits=c(0, 255))+ scale_y_reverse(limits=c(255, 0))+ theme_minimal() This is obviously glossing over a lot of exploration and cleaning, remember this should be 80% of the analysis! But it is a fair representation of the kinds of steps you might go through to get the data ready to model. Now let’s do this for all axes and clouds. img_dat = tibble(pixel=NA,type=NA,drawing=NA,pixel_number=NA) ## Axes for(i in 1:100){ tmp_draw = rjson::fromJSON(axes_json[[i]]) %&gt;% parse_drawing() grid_draw = left_join(grid_dat,tmp_draw) %&gt;% mutate(pixel = ifelse(is.na(line),0,1)) grid_draw$xgroup = cut2(grid_draw$x,g=16,levels.mean=TRUE) grid_draw$ygroup = cut2(grid_draw$y,g=16,levels.mean=TRUE) small_draw = grid_draw %&gt;% mutate(xgroup = as.numeric(as.character(xgroup)) - 7.5) %&gt;% mutate(ygroup = as.numeric(as.character(ygroup)) - 7.5) %&gt;% group_by(xgroup,ygroup) %&gt;% summarise(pixel=mean(pixel)) %&gt;% ungroup() %&gt;% select(pixel) %&gt;% mutate(type=&quot;axe&quot;,drawing=i,pixel_number=row_number()) img_dat = img_dat %&gt;% bind_rows(small_draw) } ## Clouds for(i in 1:100){ tmp_draw = rjson::fromJSON(clouds_json[[i]]) %&gt;% parse_drawing() grid_draw = left_join(grid_dat,tmp_draw) %&gt;% mutate(pixel = ifelse(is.na(line),0,1)) grid_draw$xgroup = cut2(grid_draw$x,g=16,levels.mean=TRUE) grid_draw$ygroup = cut2(grid_draw$y,g=16,levels.mean=TRUE) small_draw = grid_draw %&gt;% mutate(xgroup = as.numeric(as.character(xgroup)) - 7.5) %&gt;% mutate(ygroup = as.numeric(as.character(ygroup)) - 7.5) %&gt;% group_by(xgroup,ygroup) %&gt;% summarise(pixel=mean(pixel)) %&gt;% ungroup() %&gt;% select(pixel) %&gt;% mutate(type=&quot;cloud&quot;,drawing=i,pixel_number=row_number()) img_dat = img_dat %&gt;% bind_rows(small_draw) } library(tidyr) img_final = spread(img_dat[-1,],pixel_number,pixel) names(img_final) = c(&quot;type&quot;,&quot;drawing&quot;,paste0(&quot;pixel&quot;,1:256)) img_final ## # A tibble: 200 x 258 ## type drawing pixel1 pixel2 pixel3 pixel4 pixel5 pixel6 pixel7 pixel8 ## &lt;chr&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 axe 1 0 0 0.0232 0 0 0 0 0 ## 2 axe 2 0 0.00368 0 0 0 0 0 0 ## 3 axe 3 0.00391 0 0 0 0 0 0.00781 0 ## 4 axe 4 0 0 0 0 0 0 0 0 ## 5 axe 5 0 0.00781 0.00391 0 0 0 0 0 ## 6 axe 6 0.00368 0 0 0 0 0 0 0 ## 7 axe 7 0.00391 0.00391 0 0 0 0 0 0 ## 8 axe 8 0 0 0 0 0.00391 0.00391 0 0 ## 9 axe 9 0 0 0 0 0.00391 0 0 0 ## 10 axe 10 0 0 0 0 0 0.00368 0 0 ## # … with 190 more rows, and 248 more variables: pixel9 &lt;dbl&gt;, pixel10 &lt;dbl&gt;, ## # pixel11 &lt;dbl&gt;, pixel12 &lt;dbl&gt;, pixel13 &lt;dbl&gt;, pixel14 &lt;dbl&gt;, pixel15 &lt;dbl&gt;, ## # pixel16 &lt;dbl&gt;, pixel17 &lt;dbl&gt;, pixel18 &lt;dbl&gt;, pixel19 &lt;dbl&gt;, pixel20 &lt;dbl&gt;, ## # pixel21 &lt;dbl&gt;, pixel22 &lt;dbl&gt;, pixel23 &lt;dbl&gt;, pixel24 &lt;dbl&gt;, pixel25 &lt;dbl&gt;, ## # pixel26 &lt;dbl&gt;, pixel27 &lt;dbl&gt;, pixel28 &lt;dbl&gt;, pixel29 &lt;dbl&gt;, pixel30 &lt;dbl&gt;, ## # pixel31 &lt;dbl&gt;, pixel32 &lt;dbl&gt;, pixel33 &lt;dbl&gt;, pixel34 &lt;dbl&gt;, pixel35 &lt;dbl&gt;, ## # pixel36 &lt;dbl&gt;, pixel37 &lt;dbl&gt;, pixel38 &lt;dbl&gt;, pixel39 &lt;dbl&gt;, pixel40 &lt;dbl&gt;, ## # pixel41 &lt;dbl&gt;, pixel42 &lt;dbl&gt;, pixel43 &lt;dbl&gt;, pixel44 &lt;dbl&gt;, pixel45 &lt;dbl&gt;, ## # pixel46 &lt;dbl&gt;, pixel47 &lt;dbl&gt;, pixel48 &lt;dbl&gt;, pixel49 &lt;dbl&gt;, pixel50 &lt;dbl&gt;, ## # pixel51 &lt;dbl&gt;, pixel52 &lt;dbl&gt;, pixel53 &lt;dbl&gt;, pixel54 &lt;dbl&gt;, pixel55 &lt;dbl&gt;, ## # pixel56 &lt;dbl&gt;, pixel57 &lt;dbl&gt;, pixel58 &lt;dbl&gt;, pixel59 &lt;dbl&gt;, pixel60 &lt;dbl&gt;, ## # pixel61 &lt;dbl&gt;, pixel62 &lt;dbl&gt;, pixel63 &lt;dbl&gt;, pixel64 &lt;dbl&gt;, pixel65 &lt;dbl&gt;, ## # pixel66 &lt;dbl&gt;, pixel67 &lt;dbl&gt;, pixel68 &lt;dbl&gt;, pixel69 &lt;dbl&gt;, pixel70 &lt;dbl&gt;, ## # pixel71 &lt;dbl&gt;, pixel72 &lt;dbl&gt;, pixel73 &lt;dbl&gt;, pixel74 &lt;dbl&gt;, pixel75 &lt;dbl&gt;, ## # pixel76 &lt;dbl&gt;, pixel77 &lt;dbl&gt;, pixel78 &lt;dbl&gt;, pixel79 &lt;dbl&gt;, pixel80 &lt;dbl&gt;, ## # pixel81 &lt;dbl&gt;, pixel82 &lt;dbl&gt;, pixel83 &lt;dbl&gt;, pixel84 &lt;dbl&gt;, pixel85 &lt;dbl&gt;, ## # pixel86 &lt;dbl&gt;, pixel87 &lt;dbl&gt;, pixel88 &lt;dbl&gt;, pixel89 &lt;dbl&gt;, pixel90 &lt;dbl&gt;, ## # pixel91 &lt;dbl&gt;, pixel92 &lt;dbl&gt;, pixel93 &lt;dbl&gt;, pixel94 &lt;dbl&gt;, pixel95 &lt;dbl&gt;, ## # pixel96 &lt;dbl&gt;, pixel97 &lt;dbl&gt;, pixel98 &lt;dbl&gt;, pixel99 &lt;dbl&gt;, pixel100 &lt;dbl&gt;, ## # pixel101 &lt;dbl&gt;, pixel102 &lt;dbl&gt;, pixel103 &lt;dbl&gt;, pixel104 &lt;dbl&gt;, ## # pixel105 &lt;dbl&gt;, pixel106 &lt;dbl&gt;, pixel107 &lt;dbl&gt;, pixel108 &lt;dbl&gt;, … Since this is a prediction problem, we will need to split the data into a training and a testing set. We will learn more about the structure of machine learning vs inference problems later in the course. For now, you can just take my word for it that if we build a model in the training set, we will need a held out set of independent data to validate and critique our model. library(caret) train_set = createDataPartition(img_final$type,list=FALSE) train_dat = img_final[train_set,] test_dat = img_final[-train_set,] train_dat ## # A tibble: 100 x 258 ## type drawing pixel1 pixel2 pixel3 pixel4 pixel5 pixel6 pixel7 pixel8 ## &lt;chr&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 axe 2 0 0.00368 0 0 0 0 0 0 ## 2 axe 3 0.00391 0 0 0 0 0 0.00781 0 ## 3 axe 4 0 0 0 0 0 0 0 0 ## 4 axe 8 0 0 0 0 0.00391 0.00391 0 0 ## 5 axe 9 0 0 0 0 0.00391 0 0 0 ## 6 axe 10 0 0 0 0 0 0.00368 0 0 ## 7 axe 11 0 0 0 0 0.00781 0 0 0 ## 8 axe 13 0.00368 0 0 0 0 0.00368 0 0 ## 9 axe 14 0 0.00391 0 0 0 0 0 0 ## 10 axe 15 0.00391 0.0156 0 0.00391 0.00391 0 0 0 ## # … with 90 more rows, and 248 more variables: pixel9 &lt;dbl&gt;, pixel10 &lt;dbl&gt;, ## # pixel11 &lt;dbl&gt;, pixel12 &lt;dbl&gt;, pixel13 &lt;dbl&gt;, pixel14 &lt;dbl&gt;, pixel15 &lt;dbl&gt;, ## # pixel16 &lt;dbl&gt;, pixel17 &lt;dbl&gt;, pixel18 &lt;dbl&gt;, pixel19 &lt;dbl&gt;, pixel20 &lt;dbl&gt;, ## # pixel21 &lt;dbl&gt;, pixel22 &lt;dbl&gt;, pixel23 &lt;dbl&gt;, pixel24 &lt;dbl&gt;, pixel25 &lt;dbl&gt;, ## # pixel26 &lt;dbl&gt;, pixel27 &lt;dbl&gt;, pixel28 &lt;dbl&gt;, pixel29 &lt;dbl&gt;, pixel30 &lt;dbl&gt;, ## # pixel31 &lt;dbl&gt;, pixel32 &lt;dbl&gt;, pixel33 &lt;dbl&gt;, pixel34 &lt;dbl&gt;, pixel35 &lt;dbl&gt;, ## # pixel36 &lt;dbl&gt;, pixel37 &lt;dbl&gt;, pixel38 &lt;dbl&gt;, pixel39 &lt;dbl&gt;, pixel40 &lt;dbl&gt;, ## # pixel41 &lt;dbl&gt;, pixel42 &lt;dbl&gt;, pixel43 &lt;dbl&gt;, pixel44 &lt;dbl&gt;, pixel45 &lt;dbl&gt;, ## # pixel46 &lt;dbl&gt;, pixel47 &lt;dbl&gt;, pixel48 &lt;dbl&gt;, pixel49 &lt;dbl&gt;, pixel50 &lt;dbl&gt;, ## # pixel51 &lt;dbl&gt;, pixel52 &lt;dbl&gt;, pixel53 &lt;dbl&gt;, pixel54 &lt;dbl&gt;, pixel55 &lt;dbl&gt;, ## # pixel56 &lt;dbl&gt;, pixel57 &lt;dbl&gt;, pixel58 &lt;dbl&gt;, pixel59 &lt;dbl&gt;, pixel60 &lt;dbl&gt;, ## # pixel61 &lt;dbl&gt;, pixel62 &lt;dbl&gt;, pixel63 &lt;dbl&gt;, pixel64 &lt;dbl&gt;, pixel65 &lt;dbl&gt;, ## # pixel66 &lt;dbl&gt;, pixel67 &lt;dbl&gt;, pixel68 &lt;dbl&gt;, pixel69 &lt;dbl&gt;, pixel70 &lt;dbl&gt;, ## # pixel71 &lt;dbl&gt;, pixel72 &lt;dbl&gt;, pixel73 &lt;dbl&gt;, pixel74 &lt;dbl&gt;, pixel75 &lt;dbl&gt;, ## # pixel76 &lt;dbl&gt;, pixel77 &lt;dbl&gt;, pixel78 &lt;dbl&gt;, pixel79 &lt;dbl&gt;, pixel80 &lt;dbl&gt;, ## # pixel81 &lt;dbl&gt;, pixel82 &lt;dbl&gt;, pixel83 &lt;dbl&gt;, pixel84 &lt;dbl&gt;, pixel85 &lt;dbl&gt;, ## # pixel86 &lt;dbl&gt;, pixel87 &lt;dbl&gt;, pixel88 &lt;dbl&gt;, pixel89 &lt;dbl&gt;, pixel90 &lt;dbl&gt;, ## # pixel91 &lt;dbl&gt;, pixel92 &lt;dbl&gt;, pixel93 &lt;dbl&gt;, pixel94 &lt;dbl&gt;, pixel95 &lt;dbl&gt;, ## # pixel96 &lt;dbl&gt;, pixel97 &lt;dbl&gt;, pixel98 &lt;dbl&gt;, pixel99 &lt;dbl&gt;, pixel100 &lt;dbl&gt;, ## # pixel101 &lt;dbl&gt;, pixel102 &lt;dbl&gt;, pixel103 &lt;dbl&gt;, pixel104 &lt;dbl&gt;, ## # pixel105 &lt;dbl&gt;, pixel106 &lt;dbl&gt;, pixel107 &lt;dbl&gt;, pixel108 &lt;dbl&gt;, … We can look at a few of the pixels to see if we see any differences between the two types of images: ggplot(train_dat,aes(x=type,y=pixel1)) + geom_boxplot() + theme_minimal() ggplot(train_dat,aes(x=type,y=pixel100)) + geom_boxplot() + theme_minimal() In general, this would kick off another phase of exploratory analysis before setting off to build a model. 3.2.7 Statistical prediction/modeling Now that we have a sufficiently organized and cleaned data set we can go about modeling. This is the part that tends to get a lot of attention. Random forests, deep neural nets, and penalized regression, oh my! But the best data analyses use the simplest model that they can to get the job done. There are always tradeoffs - typically between some measure of accuracy and either cost, time, interpretability, or convenience. We will discuss these tradeoffs later in the class, but in general, it is better to err on the side of simplicity when developing a statistical model. For this simple example we will use penalized logistic regression. In particular, we use the lasso penalty - which encourages a model that uses as few of the pixels as possible when predicting. myGrid &lt;- expand.grid( alpha = 1, lambda = seq(0.0001, 1, length = 20) ) mod = train(as.factor(type) ~ . - drawing , data=train_dat, method=&quot;glmnet&quot;, tuneGrid = myGrid) confusionMatrix(as.factor(test_dat$type),predict(mod,test_dat)) ## Confusion Matrix and Statistics ## ## Reference ## Prediction axe cloud ## axe 43 7 ## cloud 7 43 ## ## Accuracy : 0.86 ## 95% CI : (0.7763, 0.9213) ## No Information Rate : 0.5 ## P-Value [Acc &gt; NIR] : 4.142e-14 ## ## Kappa : 0.72 ## ## Mcnemar&#39;s Test P-Value : 1 ## ## Sensitivity : 0.86 ## Specificity : 0.86 ## Pos Pred Value : 0.86 ## Neg Pred Value : 0.86 ## Prevalence : 0.50 ## Detection Rate : 0.43 ## Detection Prevalence : 0.50 ## Balanced Accuracy : 0.86 ## ## &#39;Positive&#39; Class : axe ## It looks like we get a reasonable level of accuracy compared to our original question - we were shooting for 80% accuracy, and for low levels of the regularization parameter we are above 80%: plot(mod) 3.2.8 Interpret results Next we might dig in deeper and try to understand why and how our model is working. For example we might look at the pixels with non-zero coefficients. library(pheatmap) coef(mod$finalModel, mod$bestTune$lambda)[-1] %&gt;% abs() %&gt;% matrix(byrow=TRUE,nrow=16) %&gt;% pheatmap(.,cluster_cols=FALSE,cluster_rows=FALSE) We see that most of the non-zero coefficients are clustered near the bottom of the image. If you look at the pictures of axes and clouds above, you’ll see that the lower part of the image often contains the handle of the axe. So one interpretation might be that the handle of the axe may be the feature our model is using to make the predictions. In a real data analysis we would want to explore and evaluate that carefully. 3.2.9 Challenge results On the one hand we have completed a “successful” data analysis - we have identified a model that answers our original, specific data science question up to our originally proposed metric for success. But there are also a number of outstanding questions like: Would this work if we had to classify many types of images? Would this level of accuracy hold up in the real world? Did our approach to downsampling produce bias? Reduce accuracy? Is our sample size sufficiently large to have confidence in our answer? Does this analysis even really address our more general question of creating a classifier for a hardware website? Some of these questions are easy to answer, some less so. But a critical component of any data analysis is identifying these issues and including them in the analysis you ultimately deliver. 3.2.10 Synthesize/write up results We have gone through a simple analysis of images from Google QuickDraw. This represents what we would call a “raw” analysis of the data. A raw analysis is performed as a stream of consciousness, trying out each idea as you go without discarding or attempting to synthesize. If this were a real data science project, the next step would be synthesis and interpretation. We will discuss the keys to a successful data analysis later in the course. But a primary purpose of an analysis is to communicate the answer to the question. So our write-up might include an executive summary of our conclusions, remove extraneous and unnecessary processing, include additional explanations for a non-technical audience, clean up figures, and provide a clearer “arc” to the analysis. We might recommend next steps for both analysis and implementation and highlight key strengths or weaknesses of our approach. 3.2.11 Create reproducible code The most common query from a collaborator, manager, or executive when you perform an analysis is: Could you just re-run that code with the [latest/different/best] parameters? Even if you are doing a data science project solely for yourself you should keep in mind that: Your closest collaborator is you six months ago, but you don’t reply to emails Both of these quotes are modified from this excellent lecture by Karl Broman as part of his Tools for Reproducible Research Course. The point of these quotes is that it is an almost certainty that you will need to run your analysis more than once. For that reason alone, it is worth it to build out code that can be used to perform the analysis automatically - saving your future self hours of frustrating investigation. 3.3 Raw, informal, and formal data science In this lecture we have performed a “raw” data analysis. We did not attempt to organize, synthesize, or format our analysis for a particular audience. Every step, piece of code, and plot inthe analysis was included whether it was relevant or not. Raw data analysis is almost always the place to start. As with the parable of the blind men and the elephant, the priority should be placed on a full exploration to identify all of the strengths and weaknesses of both the data and the methods used on the data. An “informal” data analysis takes the first steps toward polishing a data analysis for a broader audience. Common steps between a raw data analysis and an informal data analysis are: An arc of the analysis will be defined Unimportant analyses may be removed Figures may be improved Code may be removed or simplified Conclusions will be more specifically outlined But the analysis might not be subject to the formal structure of a memo, report, or journal article. Some examples of informal data analyses are: http://varianceexplained.org/r/trump-tweets/ https://hilaryparker.com/2013/01/30/hilary-the-most-poisoned-baby-name-in-us-history/ http://alyssafrazee.com/2014/06/04/gender-and-github-code.html A formal data analysis is one that may appear in a formalized report, memo, or journal. Some steps for moving from an informal to a formal data analysis are: An arc of the analysis will be streamlined Unimportant analyses will be removed Supporting analyses will be moved to supporting documentation Figures may be production quality Code will be moved to supporting documentation Methods will be referenced and supported Conclusions will be referenced and supported Moreover, the analysis will conform to a specific set of formatting rules, structure, and style that are specific to the outlet. Each of these types of data science can play an important role in scoping projects and moving them forward - depending on the relative need for speed or completeness. 3.4 Additional Resources Art of Data Science Tools for Reproducible Research Opinion: Reproducible research can still be wrong: Adopting a prevention approach 10 simple rules for structuring papers 3.5 Homework Template Repo: https://github.com/advdatasci/homework2 Repo Name: homework2-ind-yourgithubusername Pull Date: 2020/09/14 9:00AM Baltimore Time "],
["week-3.html", "4 Week 3 4.1 Week 3 Learning Objectives 4.2 Organizing a data analysis 4.3 Project Organization 4.4 File naming 4.5 Absolute vs. relative paths 4.6 Coding Variables 4.7 Project management software 4.8 Coding style 4.9 Chain of Custody for Data 4.10 Version Control 4.11 Additional Resources 4.12 Homework", " 4 Week 3 4.1 Week 3 Learning Objectives At the end of this lesson you will be able to: Organize a data analysis Use appropriate file naming Develop and apply a coding style Distinguish raw from processed data Apply the chain of custody model for data 4.2 Organizing a data analysis This week we will focus on organizing a data analysis from high level to low level. An important thing to keep in mind is that this is one system for doing this, it is not the only system for data analysis organization. The key idea here is just to be consistent in everything you do - from folder structure, to file names, to data columns, and more. This will slow you down a bit as you do your analysis! That’s ok! You will more than make up for the time somewhere down the line when you have to repeat or reproduce an analysis. As Karl Broman puts it the key steps are: Step 1: slow down and document. Step 2: have sympathy for your future self. Step 3: have a system 4.2.1 Motivation - the stick This is an advanced data science class. You’d think that the most important parts of advanced data science would focus on complicated methods like deep learning, or managing massive scale data sets. But the reality is that the primary measure of how well or poorly a data analysis goes comes down to organization and management of data, code, and writing. This is going to seem pretty simple for an advanced course, but is remarkably one of the things most commonly left out of courses. If you focus on setting your analysis well up from the start you’ll have a much better time! The classic example of what can go wrong with a poorly organized, documented data analysis is the Duke Saga. In this case, a group of scientists at Duke developed a statistical model that appeared to precisely predict patient response to chemotherapy based on gene expression measurements. This seemed like a huge precision medicine success and resulted in several high profile publications: Unfortunately, it turns out there were a lot of problems with the analysis. Some of them were very obvious problems - things like the analysts used probability incorrectly in their calculations. But many of the most important problems were things like row-names shifting due to Excel and mis-labeled values for variables (things like coding 0 = responded, 1= not responded, when it was actually the reverse). These mistakes were ultimately detailed in a statistical paper: This paper was one of the fastest reviewed in the history of statistics! The reason was that by the time this paper was published, there were already clinical trials ongoing where patients were being assigned the wrong therapy on the basis of this flawed data analysis! How could clinical trials have gotten started so quickly?! The reason is that it actually took almost 3 years (!) for the statisticians reproducing the paper to get all of the code and data together so that they could reverse engineer the problems with the analysis. It turns out there were a lot of problems with this case, from scientific, to cultural, to statistical - that led to the eventual bad clinical trial. This case was so bad that the scientists and administrators involved even faced lawsuits. Obviously most data analyses aren’t this high risk! But the real problems here were due to a lack of organization and sharing of the original research materials for the data analysis that identified the chemotherapy predictors. Had the analysts used an organized data analysis many of these problems could be avoided. If you want a highly entertaining review of the entire Duke Saga by one of the statisticians involved in identifying the errors (and one of the most entertaining statistical speakers out there) you can watch this video of a talk he gave. 4.2.2 Motivation - the carrot When you perform a data analysis the audience is either someone else or you at some time in the future. As we mentioned last week the most common query from a collaborator, manager, or executive when you perform an analysis is: Could you just re-run that code with the [latest/different/best] parameters? So even when you think you are doing an analysis for someone else, your code is actually most often just for you at some point in the future. When you first get started analyzing data you may have one project or two that you are working on. You will know where all the files are and what they all mean. But as you go on in your career, you will accumulate more analyses and each one will have a large number of code, data, and writing files. This isn’t a big problem at first, but later when someone asks you to find, reproduce, or share a piece of analysis from a paper 10 years ago, you’ll be so grateful to your past self for being organized! 4.3 Project Organization At a high level when you are organizing a data analysis project you need to manage data, code, writing, and analysis products (figures, tables, graphics). When I set up a new project I usually use a single folder with a structure like this. README.md .gitignore project.Rproj data/ raw_data/ tidy_data/ codebook.md code/ raw_code/ final_code/ figures/ exploratory_figures/ explanatory_figures/ products/ writing/ 4.3.1 README.md This is maybe the most critical piece of any data analysis that you are working on. You absolutely will forget which files were the original data, which code you used to clean the data with, that one really weird quik that means you have to run clean_data.R before preprocess_data.R and many other small details. If you keep a steady record of these changes as you go it will be much easier to reproduce, share, or manage your analysis. This could save future you hundreds or thousands of hours of repetitive and frustrating work. 4.3.2 .gitignore This file is critical. It tells you which files for Github to ignore when you are pushing to the web. The files that you should make sure you ignore are: Any data files that contain PII or HIPAA protected data Any files that contain access keys or tokens to databases or services Any big data or image files (&gt; 25 MB) It is particularly important that you make sure you don’t push access keys or private data to Github where they can be accessed publicly. Generally when I have data or keys like these I need to protect I create an additional folder project/private that contains these files. I then add _private/*_ to my .gitignore folder so that all the files in this folder are ignored. I push the .gitignore file to Github before I put anything in the private folder. Then, because I’m always paranoid about these things, I generally put a more innocuous file in the private folder and push to test to make sure it is ignored. 4.3.3 data/ 4.3.3.1 The components of a data set The work of converting the data from raw form to directly analyzable form is the first step of any data analysis. It is important to see the raw data, understand the steps in the processing pipeline, and be able to incorporate hidden sources of variability in one’s data analysis. On the other hand, for many data types, the processing steps are well documented and standardized. These are the components of a processed data set: The raw data. A tidy data set. A code book describing each variable and its values in the tidy data set. An explicit and exact recipe you used to go from 1 -&gt; 2,3 4.3.3.2 data/raw_data/ It is critical that you include the rawest form of the data that you have access to. Here are some examples of the raw form of data: The strange binary file your measurement machine spits out The unformatted Excel file with 10 worksheets the company you contracted with sent you The complicated JSON data you got from scraping the Twitter API The hand-entered numbers you collected looking through a microscope You know the raw data is in the right format if you: Ran no software on the data Did not manipulate any of the numbers in the data You did not remove any data from the data set You did not summarize the data in any way If you did any manipulation of the data at all it is not the raw form of the data. Reporting manipulated data as raw data is a very common way to slow down the analysis process, since the analyst will often have to do a forensic study of your data to figure out why the raw data looks weird. 4.3.3.3 data/processed_data The general principles of tidy data are laid out by Hadley Wickham in this paper and this video. The paper and the video are both focused on the R package, which you may or may not know how to use. Regardless the four general principles you should pay attention to are: Each variable you measure should be in one column Each different observation of that variable should be in a different row There should be one table for each “kind” of variable If you have multiple tables, they should include a column in the table that allows them to be linked While these are the hard and fast rules, there are a number of other things that will make your data set much easier to handle. First is to include a row at the top of each data table/spreadsheet that contains full row names. So if you measured age at diagnosis for patients, you would head that column with the name AgeAtDiagnosis instead of something like ADx or another abbreviation that may be hard for another person to understand. Here is an example of how this would work from genomics. Suppose that for 20 people you have collected gene expression measurements with RNA-sequencing. You have also collected demographic and clinical information about the patients including their age, treatment, and diagnosis. You would have one table/spreadsheet that contains the clinical/demographic information. It would have four columns (patient id, age, treatment, diagnosis) and 21 rows (a row with variable names, then one row for every patient). You would also have one spreadsheet for the summarized genomic data. Usually this type of data is summarized at the level of the number of counts per exon. Suppose you have 100,000 exons, then you would have a table/spreadsheet that had 21 rows (a row for gene names, and one row for each patient) and 100,001 columns (one row for patient ids and one row for each data type). If you are sharing your data with the collaborator in Excel, the tidy data should be in one Excel file per table. They should not have multiple worksheets, no macros should be applied to the data, and no columns/cells should be highlighted. Alternatively share the data in a CSV or TAB-delimited text file. 4.3.3.4 data/codebook.md For almost any data set, the measurements you calculate will need to be described in more detail than you will sneak into the spreadsheet. The code book contains this information. At minimum it should contain: Information about the variables (including units!) in the data set not contained in the tidy data Information about the summary choices you made Information about the experimental study design you used In our genomics example, the analyst would want to know what the unit of measurement for each clinical/demographic variable is (age in years, treatment by name/dose, level of diagnosis and how heterogeneous). They would also want to know how you picked the exons you used for summarizing the genomic data (UCSC/Ensembl, etc.). They would also want to know any other information about how you did the data collection/study design. For example, are these the first 20 patients that walked into the clinic? Are they 20 highly selected patients by some characteristic like age? Are they randomized to treatments? A common format for this document is a Word file. There should be a section called “Study design” that has a thorough description of how you collected the data. There is a section called “Code book” that describes each variable and its units. 4.3.3.5 The instruction list/script You may have heard this before, but reproducibility is kind of a big deal in computational science. That means, when you submit your paper, the reviewers and the rest of the world should be able to exactly replicate the analyses from raw data all the way to final results. If you are trying to be efficient, you will likely perform some summarization/data analysis steps before the data can be considered tidy. The ideal thing for you to do when performing summarization is to create a computer script (in R, Python, or something else) that takes the raw data as input and produces the tidy data you are sharing as output. You can try running your script a couple of times and see if the code produces the same output. In many cases, the person who collected the data has incentive to make it tidy for a statistician to speed the process of collaboration. They may not know how to code in a scripting language. In that case, what you should provide the statistician is something called pseudocode. It should look something like: Step 1 - take the raw file, run version 3.1.2 of summarize software with parameters a=1, b=2, c=3 Step 2 - run the software separately for each sample Step 3 - take column three of outputfile.out for each sample and that is the corresponding row in the output data set You should also include information about which system (Mac/Windows/Linux) you used the software on and whether you tried it more than once to confirm it gave the same results. Ideally, you will run this by a fellow student/labmate to confirm that they can obtain the same output file you did. 4.3.4 code/ 4.3.4.1 code/raw_code When you start out doing an analysis recall that you will be doing exploration and data cleaning. Unless you have worked with this type of data before, you will be acting like the blind men and the elephant - just trying to figure out the different aspects of the data set. The important characteristics of this stage of an analysis are speed and comprehensiveness. You will want to make many plots, tables, and summaries of the data. You will generally not be trying to make these clean or organize the results. This is ok! You can put these (often slightly unorganized) R scripts in the code/raw_code folder. While these scripts are generally unorganized two things will save you a lot of time later: Make sure you comment your code, even if it is really fast/simple comments. Especially highlight any important discoveries/plots you think might be useful later with labeled comments. If you find anything really important that you definitely will include in downstream analysis add a note to your README.md file about the nature of the discovery and which file you discovered it in, so you can find it later. 4.3.4.2 code/final_code Your raw code is mostly for you - to help you figure out a data set, start to scope your analysis, and organize your thoughts. Your final code is for other people or future you. Final code should be organized, easy to follow, and reproducible. Final code is like a curation of your original raw code. You shouldn’t keep every plot, figure, summary or table from your raw code in your final code. Only the ones that are important and are part of the story. Some important principles of final code are: Files should be ordered so it is easy to follow the flow of an analysis (see file naming section for more) You should use relative, rather than absolute, paths for all coding within your final code (see section on relative vs absolute paths below) If you randomly sample data, or use methods like k means clustering or random forests that are inherently stochastic, you should set a seed so that calculations can be reproduced exactly. Final code files should either be in R markdown format or heavily commented so it is easy to follow all the steps in ana analysis. 4.3.5 figures We will have an entire week focused on just figures, so for now we will briefly explain these two folders, but more will be coming soon! 4.3.5.1 figures/exploratory_figures Exploratory figures are figures that are made rapidly to determine the structure, quirks, and summaries of the data that will be important for downstream analysis. The important thing about exploratory figures is that you should make a lot of them! Exploratory figures are like raw code, they are mostly for you. You should save any figure you make that you think could be useful for you later. You should still maintain good file naming practices and include comments in your raw code that point out which code produces which exploratory figure. 4.3.5.2 figures/explanatory_figures Explanatory figures, like final code, is for sharing with others. You will generally make many fewer explantory figures than exploratory figures, they will generally be much more polished, and you will want to make sure that they clearly follow the “arc” of the data analysis you are performing. Since this is a folder you will be sharing with others, it is a good idea to include documentation in your README.md explaining what these figures are and which code in the code/final_code folder makes these figures. 4.3.6 products/ This folder is for any complete data analytic products you will be sharing with your collaborators, manager, or audience. For our class, we will be primarily focusing on written data analyses so the main folder we include is products/writing but you may also include folders such as products/presentations, products/shinyapps, or products/dashboards if they are more appropriate for the type of sharing you are performing. 4.3.6.1 products/writing This folder will contain written documents you are sharing with collaborators. They may be Word documents, links to Google Docs, or Latex documents. Sometimes it will be possible for one of your .Rmd files to compile directly to these documents, but it is more common that these will be passed back and forth with collaborators. It is good to separate these writing files from those that you are managing and organizing for the broader data analysis. 4.4 File naming Jenny Bryan, one of the world’s best data scientists says that: File organization and naming are powerful weapons against chaos. If this looks familiar, you may want to update your file naming approach! She has outlined some key principles for naming files. While these principles are especially useful for final code, they are also useful for naming data files, for naming raw code, and for naming figures and writing. The principles are that files should be: Machine readable Human readable Be nicely ordered Machine readable means that file names shouldn’t contain spaces, special characters, and should include important pieces of information about the file contents (sometimes called slugs) separated by underscores or dashes so that it is easy to run computer code to search for specific files. It is often easier if file names are entirely lowercase letters to avoid simple coding errors. Human readable means that files should be labeled with names that make it easy for a person to follow along. So err on the side of long file names rather than abbreviations, numeric only file names, or obscure file names. When you open a folder - especially the final code folder - it is useful to see the files in the order you want them to be run. One way to do this is to name files alpha-numerically so that they will appear in the right order on your computer. One way to do this is to order the files and then append a number to the beginning of each file name (like 01_data_cleaning.R, 02_exploratory_analysis.R etc. so that the files will be ordered correctly). Here are some examples of good and bad names for files - can you figure out why the bad file names are bad? As an example, here are two data files from an analysis I worked on. processed_pvalue_data_from_pubmed_oct24.rData raw_pvalue_data_from_pubmed_oct24.rData The first part of the file name tells you what I did to the data: processed_pvalue_data_from_pubmed_oct24.rData raw_pvalue_data_from_pubmed_oct24.rData the second part of the file name tells you what kind of data it is: processed_pvalue_data_from_pubmed_oct24.rData raw_pvalue_data_from_pubmed_oct24.rData and the last part tells you the date I pulled the data: processed_pvalue_data_from_pubmed_oct24.rData raw_pvalue_data_from_pubmed_oct24.rData These file names are human readable, machine readable (you can sort by processing type, etc.), and they are nicely ordered if you care about the data processing as the main ordering variable. 4.5 Absolute vs. relative paths An important reason you are organizing your data analysis files is that you will often have to share the files with others. One thing to keep in mind is that when you share files with others they are no longer on your computer. This means that if you give your code to someone else, they will need to be able to find the files, data, and code within the folders you have created inside your project. Imagine for example that you have set up your folder structure like this: The reality is that while you will share only this one folder with someone else; it is on your computer somewhere and might be buried deep in your folder structure. The important thing to realize is you have to specify files you call in your analysis so they can be found by others. So if you source a file like this: source('C:\\Documents\\ds_projects\\project\\code\\final_code\\myfile.R') source('/root/analyses/ds_projects/project/code/final_code/myfile.R') Then it won’t work on someone else’s computer since they don’t have that same file structure! To avoid this, you should use relative paths, which specify the file path to a folder relative to the project folder. You can do that with the here package in R. The here package looks for a .Rproj file and points to that directory: The nice thing about the here function finds the base folder regardless of which folder you are in inside the project directory: You can use the here function to define paths in your code to code or data files: So for example if your working directory is /root/analyses/ds_projects/project/ and you run the code: here('code/final_code/myfile.R') it will produce the file path: /root/analyses/ds_projects/project/code/final_code/myfile.R and if you are on a different computer and your working directory is C:\\Documents\\ds_projects\\ and you run the code: here('code/final_code/myfile.R') it will produce the file path: C:\\Documents\\ds_projects\\project\\code\\final_code\\myfile.R In other words, here will produce the complete path to your myfile.R file, relative to the directory where you have stored .Rproj. This means that regardless of where a person puts the folder on their computer, the here function will be able to create the path to the data. The optimal way to take advantage of this package is to use Rstudio projects but you can use the set_here() function to create a .here file that will function as if it was the .Rproj file for targeting the here function. 4.6 Coding Variables When you put variables into a spreadsheet there are several main categories you will run into depending on their data type: Continuous Ordinal Categorical Missing Censored Continuous variables are anything measured on a quantitative scale that could be any fractional number. An example would be something like weight measured in kg. Ordinal data are data that have a fixed, small (&lt; 100) number of levels but are ordered. This could be for example survey responses where the choices are: poor, fair, good. Categorical data are data where there are multiple categories, but they aren’t ordered. One example would be sex: male or female. Missing data are data that are missing and you don’t know the mechanism. You should code missing values as NA. Censored data/make up/ throw away missing observations. In general, try to avoid coding categorical or ordinal variables as numbers. When you enter the value for sex in the tidy data, it should be “male” or “female”. The ordinal values in the data set should be “poor”, “fair”, and “good” not 1, 2 ,3. This will avoid potential mixups about which direction effects go and will help identify coding errors. Always encode every piece of information about your observations using text. For example, if you are storing data in Excel and use a form of colored text or cell background formatting to indicate information about an observation (“red variable entries were observed in experiment 1.”) then this information will not be exported (and will be lost!) when the data is exported as raw text. Every piece of data should be encoded as actual text that can be exported. One very good approach to naming columns in a data analysis are with multiple levels as described in this great blog post by Emily Riederer. For example your variable names might include: A slug for the type of variable (ID for ids, IND for indicators, N for counts, etc.) A slug for subjects being measured (DRIVER for drivers, RIDER for riders etc.) A slug for what is being measured (CITY, ZIPCODE, etc) which turns into data column names like: ID_DRIVER N_TRIP_PASSENGER_ORIGIN_DESTINATION The blog post contains much more information on this approach. 4.7 Project management software The approach we discussed here to managing a project can be done by hand or by copy pasting each time you create a new project. It has some advantages - it is lightweight and focused on organization - so you don’t depend on any outside software. That being said there are some important pieces of software that have been developed for project management in R. These packages can help automate some of the processes that make data project management difficult. drake- is software for automating pipelines in R, it is smart about knowing which file to run in which order and can be really useful if you have a complicated set of processing scripts that must be run in just the right order to produce your final analysis. workflowr - is a package for setting up and automating some of the file organization/folder structure described in this lecture. It also contains workflow management functions and can be combined with drake to make workflows automated. snakemake is more general purpose workflow automation software. If you are doing lots of processing outside of R, this is probably more useful than drake. 4.8 Coding style While it isn’t as critical as some of the other organizing principles we have discussed here, having clean code is really helpful for making it easier for others to follow your analysis. As with many of the other suggestions in this week’s lesson, the key principle here is consistency. There are a variety of different coding style definitions you can go with - if you use a popular framework your code will be more easily readable by individuals from that community. Google’s R Style Guide Tidyverse Style Guide Bioconductor Style Guide 4.9 Chain of Custody for Data “Raw data” is one of those terms that everyone in statistics and data science uses but no one defines. For example, we all agree that we should be able to recreate results in scientific papers from the raw data and the code for that paper. But what do we mean when we say raw data? When working with collaborators or students I often find myself saying - could you just give me the raw data so I can do the normalization or processing myself. To give a concrete example, I work in the analysis of data from high-throughput genomic sequencing experiments. These experiments produce data by breaking up genomic molecules into short fragements of DNA - then reading off parts of those fragments to generate “reads” - usually 100 to 200 letters long per read. But the reads are just puzzle pieces that need to be fit back together and then quantified to produce measurements on DNA variation or gene expression abundances. High throughput sequencing Image from Hector Corrata Bravo’s lecture notes When I say “raw data” when talking to a collaborator I mean the reads that are reported from the sequencing machine. To me that is the rawest form of the data I will look at. But to generate those reads the sequencing machine first (1) created a set of images for each letter in the sequence of reads, (2) measured the color at the spots on that image to get the quantitative measurement of which letter, and (3) calculated which letter was there with a confidence measure. The raw data I ask for only includes the confidence measure and the sequence of letters itself, but ignores the images and the colors extracted from them (steps 1 and 2). So to me the “raw data” is the files of reads. But to the people who produce the machine for sequencing the raw data may be the images or the color data. To my collaborator the raw data may be the quantitative measurements I calculate from the reads. When thinking about this I realized an important characteristics of raw data. Raw data is relative to your reference frame. In other words the raw data is raw to you if you have done no processing, manipulation, coding, or analysis of the data. In other words, the file you received from the person before you is untouched. But it may not be the rawest version of the data. The person who gave you the raw data may have done some computations. They have a different “raw data set”. The implication for reproducibility and replicability is that we need a “chain of custody”. The chain of custody model states that each person must keep a copy of the raw data and a complete record of the operations they performed to arrive at their version of processed data. As long as each person follows these steps, then the data will be As long as each person keeps a copy and record of the “raw data” to them you can trace the provencance of the data back to the original source. 4.10 Version Control Typically when you are working on data analysis files, you will have to update them over and over again. One way to handle this is to save a new file each time like in this comic from PhD Comics This is a ….bad idea. You should instead use a version control system that will keep track of the changes to your files. The unfortunate truth about version control though is that most version control systems can be frustrating: “Version control is a truly vital concept that has unfortunately been implemented by madmen.” Amen. https://t.co/wSI7r9Epm7 — mike cook (@mtrc) July 3, 2015 It is as an assumed pre-requisite for this class that you know how to use a version control system like Github. That being said there are a number of good tutorials out there for using Git/Github both with R and more generally. I highly recommend, for example, that you check out Happy Git and Github for the UseR. 4.11 Additional Resources Organizing Data Science Projects Happy Git and Github for the UseR How to name files Coordinating with collaborators 4.12 Homework Template Repo: https://github.com/advdatasci/homework3 Repo Name: homework3-ind-yourgithubusername Pull Date: 2020/09/21 9:00AM Baltimore Time "]
]
