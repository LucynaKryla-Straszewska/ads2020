[
["index.html", "Advanced Data Science 2020 1 Welcome and Syllabus 1.1 Assumptions and pre-requisites 1.2 Learning Objectives 1.3 Course Staff 1.4 Course logistics 1.5 Assignment Due Dates 1.6 The Pandemic 1.7 Grading 1.8 Assignments 1.9 Code of Conduct 1.10 Academic Ethics 1.11 Disability support services 1.12 Email alerts 1.13 Previous versions of the class 1.14 Typos and corrections", " Advanced Data Science 2020 Jeff Leek and Roger D. Peng 2020-08-31 1 Welcome and Syllabus Welcome! We are very excited to have you in our two-term (one semester) course on Advanced Data Science with course numbers 140.711 and 140.712 offered by the Department of Biostatistics at the Johns Hopkins Bloomberg School of Public Health. This course is designed for PhD students at Johns Hopkins Bloomberg School of Public Health. We are usually pretty flexible about permitting outside students but we want everyone to be aware of the goals and assumptions so no one feels like they are surprised by how the class works. The primary goal of the course is to teach you how to deconstruct, perform, and communicate professional data analyses across diverse media. The class is not designed to teach a set of statistical methods or packages - there are a ton of awesome classes, books, and tutorials about those things out there! Rather the goal is to help you to organize your thinking around how to combine the things you have learned about statistics, data manipulation, and visualization into complete data analyses that answer important questions about the world around you. 1.1 Assumptions and pre-requisites The course is designed for PhD students in the Johns Hopkins Biostatistics Masters and PhD programs and assumes significant background in statistics. Specifically we assume: You know the basics of statistics The central dogma (estimates, standard errors, basic distributions, etc.) Key statistical terms and methods Estimtion vs testing vs prediction You know how to fit and interpret statistical models Linear Models Generalized Linear Models Smoothing splines Basic mixture models You know the basics of R or Python You can read in, clean, tidy data You can fit models You can make visualizations You know the basics of reproducible research You know what version control is You know how to use Github You know how to use R/Rmarkdown Since the target audience for this course is advanced students in statistics we will not be able to spend significant time covering these concepts and technologies. To give you some idea about how these prerequisites will impact your experience in the course, we will be turning in all assignments via R markdown documents submitted through Github pull requests. The majority of the assignments will involve critiquing, fitting and interpreting statistical analyses - primarily focused on regression. Data analyses you will perform will also often involve significant data extraction, cleaning, and transformation. Hopefully all of that sounds familiar to you so you can focus on the concepts we will be teaching around deconstructing and constructing data analyses. Some resources that may be useful if you feel you may be missing pieces of this background: Statistics - Mathematical Biostatistics Bootcamp I (Coursera); Mathematical Biostatistics Bootcamp II (Coursera) Basic Data Science - Cloud Data Science (Leanpub); Data Science Specialization (Coursera) Version Control - Github Learning Lab; Happy Git and Github for the useR Rmarkdown - Rmarkdown introduction 1.2 Learning Objectives Our goal is by the end of our class: You will be able to critique a data analysis and separate good from bad analysis. Specifically you will be able to: Identify the underlying question Evaluate the “arc” of the data analysis Identify the underlying type of question Identify the study design Determine if visualizations are appropriate Determine if methods are appropriate Identify pipeline issues Identify reproducibility issues Identify common fallacies and mistakes Distinguish what is a real problem from what is just hard Identify common fallacies and mistakes. Evaluate the relationship between study design, data, and claims to data justification You will be able to produce a complete data analysis. Specifically you will learn to: Translate general questions to data analysis questions Explore your data skeptically Select appropriate data analytic tools given the study design Combine appropriate data analytic tools into pipelines Identify strengths and weaknesses of data pipelines you produce Describe the results of your analysis accurately Decide what is and is not relevant to the “arc” of the data analysis Write the “arc” of the data analysis Avoid “reinventing the wheel” You will be able to produce the components of a data analytic paper: The “arc” of a data analysis Abstracts Introductions Figures Tables Methods sections Discussion/limitations sections You will be able to produce the components of a methods paper: The “arc” of a methods paper Abstracts Introductions Figures Tables Simulation sections Applications sections Discussion/limitations sections You will be able to produce the components of a data analytic presentation for technical and non-technical audiences: Problem introduction Methods Results Conclusions You will be able to identify key issues in data analytic relationships. Specifically you will be able to: Elicit objective functions from collaborators Identify types of data analysis relationships (collaboration, consultation, employment) Identify successful stategies for data analysis based on relationship type Identify key ethical issues in data analysis Understand your responsibility as a data analyst Explain the value of data science to non-technical audiences 1.3 Course Staff The course instructors this year are Jeff Leek and Roger Peng. We are both professors in the Biostatistics Department at Johns Hopkins and Directors of the Johns Hopkins Data Science Lab. Jeff’s research focuses on human genomics, meta-research, and edtech for social good. Roger’s research focuses on air pollution, spatial statistics, and reproducibility. We have been friends for about 10 years and are excited to teach you some of the ins and outs of data science. We also have a couple of amazing TA’s this year: Eric Bridgeford who works on independence testing, manifold embedding, and graph inference; and Athena Chen work works on developing statistical tools to analyzing proteomic and genomic data to facilitate a deeper understanding of disease. 1.4 Course logistics This is a pretty unusual year because we will be entirely online. So our logistics will be a little different than usual. The course webpage will be here at: http://jtleek.com/ads2020/ All communication for the course is going to take place on one of four platforms: Slack - for discussion, sharing resources, collaborating, and announcements - Course slack channel: jhsph-ads-2020.slack.com Github - for submitting assignments - Course Github: https://github.com/jtleek/ads2020 Zoom - for live class discussions - Course Zoom: Link available on Course Slack Hypothesis for annotating/reviewing data analyses The primary communication for the class will go through Slack. That is where we will post course announcements, post all assignments, host most of our asynchronous course discussion, and as the primary means of communication between course participants and course instructors. You should request access to the JHU Advanced Data Science Course Slack immediately. The course TA’s will approve your access. Once you have access you will also be able to find the course Zoom and Zoom password. We will have two synchronous meetings a week for discussion (see section on Discussions below) - the class will be split into two approximately equal groups for these sessions: Available Times: - Mondays 9-10AM Baltimore Time - Mondays 1:30-2:30PM Baltimore Time Location: Zoom - link available on Slack For people who miss the sessions we will try to have a recap and notes that we will post to Slack so people can read them offline. If you haven’t already, please fill out the pre-course survey with your information and your preferred discussion time. 1.5 Assignment Due Dates All course assignment due dates will appear on the weekly course chapter. Please refer to these chapters for due dates. 1.6 The Pandemic This is how 2020 feels: It is super tough to be dealing with a pandemic, an economic crisis, challenges with visas and travel and coordinating school online. Your instructors understand that this is not an ordinary year. We are ultra sympathetic to family challenges and life challenges. We both have small children at home (who may make cameos in class discussions). Our goal is to make as much of the class asynchronous as possible so you can work whenever you have time, our plan is to be as understanding as possible when it comes to grading attendance, and any issues that come up with the course. Please don’t hesitate to reach out to us if you are having issues and we will do our best to direct you to whatever resources we have/accomodate you however we can. We think the material in this course is important, fun, and this is an opportunity to learn a lot. But life is more important than a course and if there was ever a year that life might get in the way of learning, this is that year. Good enough is the excellence of 2020. 1.7 Grading 1.7.1 Philosophy We believe the purpose of graduate education is to train you to be able to think for yourself and initiate and complete your own projects. We are super excited to talk to you about ideas, work out solutions with you, and help you to figure out how to produce professional data analyses. We don’t think that graduate school grades are important for this purpose. This means that we don’t care very much about graduate student grades. That being said, we have to give you a grade so they will be: A - Excellent - 90%+ B - Passing - 80%+ C - Needs improvement - 70%+ We rarely give out grades below a C and if you consistently submit work, participate in discussions, and do your best you are very likely to get an A or a B in the course. 1.7.2 Relative weights This course is primarily focused on deconstructing and constructing data analyses. The grading will be based on your participation in the course and helping each other improve your data analyses. The breakdown of grading will be: 40% for completing required reviews - see section on reviews below 40% for completing required data analysis assignments - see section on data analysis assignments below 20% for course participation on Slack and Zoom - see section on class participation below If you submit each review, it is your own work, and it meets a basic level of completeness and effort you will get 100% for that review. If you submit a review but it doesn’t meet basic completeness and effort you will receive 50%. If you do not submit a review you will receive 0%. If you submit a data analysis assignment, it is your own work, and it meets a basic level of completeness and effort you will get 100% for that data analysis assignment. If you submit a data analysis assignment but it doesn’t meet basic completeness and effort you will receive 50%. If you do not submit a review you will receive 0%. Grading participation is difficult in the best of circumstance and in a pandemic it is basically impossible. If you are at 80% of your assigned discussion sessions and participate in the discussion most of the time and respond to Slack prompts at least 5 times during the course of the term you will receive full participation points. If this level of course participation is challenging for you please reach out to the course instructors and we will work with you to figure out how to ensure you can participate sufficiently to get full points. 1.8 Assignments 1.8.1 Submitting assignments You will be invited to the JHSPH Advanced Data Science course organization: https://github.com/advdatasci. There will be one repo for each assignment. You will see two copies - the template repository (public) and your assigned homework repo which will be suffixed with your github user name (private). The assignment repos will include an Rmd file each week for you to fill out. For each assignment we will provide a time when we will pull your changes from Github. We will assume whatever version we pull at that time is what you are turning in. When we start peer reviewing each other’s work, after the submission deadline you will also be assigned another repo (private) with a suffix of your github user name and “peer-review”. This repo will include your peer’s work and a reviewing form which we will ask you to fill out and submit. After peer review is completed the feedback will be returned to you and you will be able to pull those changes to your computer, fill out a reviewer feedback form, and push your changes back to Github. Instruction submissions will be included with each assignment to remind you of the process you need to take and what dates/times to complete assignments. 1.8.2 Data Analysis Assignments After we have spent a few weeks reviewing data analytic work written by others, we will begin working on data analyses within 1.8.3 Data Analysis Reviews In this course we will be reviewing both published data analyses and each other’s work. You will review both in writing and orally during course discussions. Reviews in our course will take the following format: Written reviews For each assigned data analysis you will provide a written review which will include a summary of the data analysis and answers to key questions. At the beginning of the term these reviews will focus on published/public data analyses, but once we begin to turn in data analysis assignments, they will be peer reviews of each other’s work. Oral reviews The course will be broke up into two groups. Each group will meet once a week to discuss the papers/data analyses for that week. Each week, each group will have 2 discussion leaders: Lead Reviewer 1 and Lead Reviewer 2. We will rotate so each person gets to be a lead reviewer. Your responsibilities as lead reviewer are to: Lead Reviewer 1: Complete your written review and provide an overall summary of the data analysis and your answers to the questions. You are responsible for leading the discussion of the analysis. Lead Reviewer 2: Complete your written review and provide a second opinon on the data analysis, either supporting or providing new viewpoints of Reviewer 1. Both reviewers are encouraged to lead discussion of the data analysis to get feedback from the other participants in the session. 1.8.4 Reviewing Code of Conduct We will be reviewing both public work and each other’s work in both written and oral form. Reviewing well is an art form and is an important skill to master - and not just for this course! Regardless of where you go after Hopkins, you will be tasked with reviewing the work of others. The key principles of doing a good job in reviewing and the foundation for our course code of conduct are: Being concise - nothing extraneous Being precise - stating the specific problems with the manuscript or data analysis Being honest - stating any real issues you perceive Being constructive - stating how the authors could address the problems you have found Being polite - this helps focus on real issues rather than pet peeves. Reviewing each other’s work well is a critical challenge. Remember that there is a person behind the data analysis and you want them to improve. It is very easy to be sucked into the temptation to write a review that is entirely critical or even rude. The best reviews follow the guidelines above and are short, percise, documents that politely suggest constructive critiques. It takes practice to produce these kinds of reviews, which we will work on in class! One of the biggest privileges is the priveledge to say you don’t know or that you need something explained. I use this priveledge all the time and it makes it much easier for me when I’m trying to learn new concepts. I want you all to feel that privilege in this class. It is critical that we are able to have discussions in the class and everyone can voice their opinion without feeling looked down on. So let’s work together to allow everyone space to learn maximally. An amazing benefit of my privilege is being able to say “I didn't understand that. Could you explain it again?” as many times as necessary without having to worry that people will think I'm stupid. — Arvind Narayanan (@random_walker) August 26, 2020 Jeff has previously written a guide for written reviews of papers. 1.9 Code of Conduct We are committed to providing a welcoming, inclusive, and harassment-free experience for everyone, regardless of gender, gender identity and expression, age, sexual orientation, disability, physical appearance, body size, race, ethnicity, religion (or lack thereof), political beliefs/leanings, or technology choices. We do not tolerate harassment of course participants in any form. Sexual language and imagery is not appropriate for any work event, including group meetings, conferences, talks, parties, Twitter and other online media. This code of conduct applies to all course participants, including instructors and TAs, and applies to all modes of interaction, both in-person and online, including GitHub project repos, Slack channels, and Twitter. Course participants violating these rules will be referred to leadership of the Department of Biostatistics and the Title IX coordinator at JHU and may face expulsion from the class. All class participants agree to: Be considerate in speech and actions, and actively seek to acknowledge and respect the boundaries of other members. Be respectful. Disagreements happen, but do not require poor behavior or poor manners. Frustration is inevitable, but it should never turn into a personal attack. A community where people feel uncomfortable or threatened is not a productive one. Course participants should be respectful both of the other course participants and those outside the course. Refrain from demeaning, discriminatory, or harassing behavior and speech. Harassment includes, but is not limited to: deliberate intimidation; stalking; unwanted photography or recording; sustained or willful disruption of talks or other events; inappropriate physical contact; use of sexual or discriminatory imagery, comments, or jokes; and unwelcome sexual attention. If you feel that someone has harassed you or otherwise treated you inappropriately, please alert Jeff Leek or Roger Peng. Take care of each other. Refrain from advocating for, or encouraging, any of the above behavior. And, if someone asks you to stop, then stop. Alert Jeff Leek or Roger Peng if you notice a dangerous situation, someone in distress, or violations of this code of conduct, even if they seem inconsequential. 1.9.1 Need Help? Please speak with Jeff Leek or Roger Peng. You can also reach out to Karen Bandeen-Roche, chair of the department of Biostatistics or Margaret Taub, Ombudsman for the Department of Biostatistics. You may also reach out to any Hopkins resource for sexual harassment, discrimination, or misconduct: JHU Sexual Assault Helpline, 410-516-7333 (confidential) University Sexual Assault Response and Prevention website Johns Hopkins Compliance Hotline, 844-SPEAK2US (844-733-2528) Hopkins Policies Online JHU Office of Institutional Equity 410-516-8075 (nonconfidential) Johns Hopkins Student Assistance Program (JHSAP), 443-287-7000 University Health Services, 410-955-1892 The Faculty and Staff Assistance Program (FASAP), 443-997-7000 1.9.2 Feedback We welcome feedback on this Code of Conduct. 1.9.3 License and attribution This Code of Conduct is distributed under a CC-BY license. Portions of above text comprised of language from the Codes of Conduct adopted by rOpenSci and Django, which are licensed by CC BY-SA 4.0 and CC BY 3.0. This work was further inspired by Ada Initiative’s “how to design a code of conduct for your community” and Geek Feminism’s Code of conduct evaluations and expanded by Ashley Johnson and Shannon Ellis in the Leek group. 1.10 Academic Ethics Students enrolled in the Bloomberg School of Public Health of The Johns Hopkins University assume an obligation to conduct themselves in a manner appropriate to the University’s mission as an institution of higher education. A student is obligated to refrain from acts which he or she knows, or under the circumstances has reason to know, impair the academic integrity of the University. Violations of academic integrity include, but are not limited to: cheating; plagiarism; knowingly furnishing false information to any agent of the University for inclusion in the academic record; violation of the rights and welfare of animal or human subjects in research; and misconduct as a member of either School or University committees or recognized groups or organizations. Students should be familiar with the policies and procedures specified under Policy and Procedure Manual Student-01 (Academic Ethics), available on the school’s portal. The faculty, staff and students of the Bloomberg School of Public Health and the Johns Hopkins University have the shared responsibility to conduct themselves in a manner that upholds the law and respects the rights of others. Students enrolled in the School are subject to the Student Conduct Code (detailed in Policy and Procedure Manual Student-06) and assume an obligation to conduct themselves in a manner which upholds the law and respects the rights of others. They are responsible for maintaining the academic integrity of the institution and for preserving an environment conducive to the safe pursuit of the School’s educational, research, and professional practice missions. 1.11 Disability support services If you are a student with a documented disability who requires an academic accommodation, please contact the Office of Disability Support Services at 410-502-6602 or via email at JHSPH.dss@jhu.edu. Accommodations take effect upon approval and apply to the remainder of the time for which a student is registered and enrolled at the Bloomberg School of Public Health. 1.12 Email alerts The full course content will be available via this website. All assignments will be posted here and on Slack. But we are trying an experiment with Substack and you can sign up here for email alerts when new course chapters are available: https://jhuadvdatasci.substack.com/. If you aren’t at JHU but want to follow along with the content you are welcome to sign up as well! 1.13 Previous versions of the class https://jhu-advdatasci.github.io/2019/ https://jhu-advdatasci.github.io/2018/ http://jtleek.com/advdatasci/ http://jtleek.com/advdatasci16/ http://jtleek.com/advdatasci15/ https://github.com/jtleek/jhsph753and4 1.14 Typos and corrections Feel free to submit typos/errors/etc via the github repository associated with the class: https://github.com/jtleek/ads2020. You will have the thanks of your grateful instructors! "],
["week-1.html", "2 Week 1 2.1 Learning objectives 2.2 What is advanced data science anyway? 2.3 Types of data analytic questions 2.4 A data analytic rubric 2.5 Your first assignment - deconstructing an analysis 2.6 Additional Resources 2.7 Homework", " 2 Week 1 2.1 Learning objectives At the end of this lesson you will: Be able to define data science and advanced data science Be able to define the types of data analytic questions Be able to follow a data analysis rubric to evaluate an analysis 2.2 What is advanced data science anyway? 2.2.1 Maybe we should start by defining data science…. Before we can define advanced data science we need to define data science. The definition we will use is: Data science is the process of formulating a quantitative question that can be answered with data, collecting and cleaning the data, analyzing the data, and communicating the answer to the question to a relevant audience. In general the data science process is iterative and the different components blend together a little bit. But for simplicity lets discretize the tasks into the following 7 steps: Define the question of interest Get the data Clean the data Explore the data Fit statistical models Communicate the results Make your analysis reproducible The reality is that the process is usually much more iterative. This is an excellent diagram describing the usual flow of a data science project by a former student in this class, Simina Boca: Feeling preeety good about this diagram that I wrote in sparkly pens for the data analysis class I'm teaching, which starts tomorrow… Hope it's clear now that data scientists and applied statisticians don't simply press a 🖱️or wave a 🪄! Feedback welcome for future iterations! pic.twitter.com/BoDeyUuNvT — Simina M. Boca (@siminaboca) August 27, 2020 A good data science project answers a real scientific or business analytics question. In almost all of these experiments the vast majority of the analyst’s time is spent on getting and cleaning the data (steps 2-3) and communication and reproducibility (6-7). In most cases, if the data scientist has done her job right the statistical models don’t need to be incredibly complicated to identify the important relationships the project is trying to find. In fact, if a complicated statistical model seems necessary, it often means that you don’t have the right data to answer the question you really want to answer. As Tukey said: The combination of some data and an aching desire for an answer does not ensure that a reasonable answer can be extracted from a given body of data. One option is to spend a huge amount of time trying to tune a statistical model to try to answer the question but serious data scientist’s usually instead try to go back and get the right data. The result of this process is that most well executed and successful data science projects don’t (a) use super complicated tools or (b) fit super complicated statistical models. The characteristics of the most successful data science projects I’ve evaluated or been a part of are: (a) a laser focus on solving the scientific problem, (b) careful and thoughtful consideration of whether the data is the right data and whether there are any lurking confounders or biases and (c) relatively simple statistical models applied and interpreted skeptically. 2.2.2 Data Science is hard, but not like math is hard It turns out doing those three things is actually surprisingly hard and very, very time consuming. It is my experience that data science projects take a solid 2-3 times as long to complete as a project in theoretical statistics. The reason is that inevitably the data are a mess and you have to clean them up, then you find out the data aren’t quite what you wanted to answer the question, so you go find a new data set and clean it up, etc. After a ton of work like that, you have a nice set of data to which you fit simple statistical models and then it looks super easy to someone who either doesn’t know about the data collection and cleaning process or doesn’t care. This poses a major public relations problem for serious data scientists. When you show someone a good data science project they almost invariably think “oh that is easy” or “that is just a trivial statistical/machine learning model” and don’t see all of the work that goes into solving the real problems in data science. A concrete example of this is in academic statistics. It is customary for people to show theorems in their talks and maybe even some of the proof. This gives people working on theoretical projects an opportunity to “show their stuff” and demonstrate how good they are. The equivalent for a data scientist would be showing how they found and cleaned multiple data sets, merged them together, checked for biases, and arrived at a simplified data set. Showing the “proof” would be equivalent to showing how they matched IDs. These things often don’t look nearly as impressive in talks, particularly if the audience doesn’t have experience with how incredibly delicate real data analysis is. I imagine versions of this problem play out in industry as well (candidate X did a good analysis but it wasn’t anything special, candidate Y used Hadoop to do BIG DATA!). The really tricky twist is that bad data science looks easy too. You can scrape a data set off the web and slap a machine learning algorithm on it no problem. So how do you judge whether a data science project is really “hard” and whether the data scientist is an expert? Just like with anything, there is no easy shortcut to evaluating data science projects. You have to ask questions about the details of how the data were collected, what kind of biases might exist, why they picked one data set over another, etc. In the meantime, don’t be fooled by what looks like simple data science - it can often be pretty effective. This course is designed for PhD students in Biostatistics and most of the courses you have taken have been hard by virtue of mathematical difficulty. These courses focus on deductive resasoning whereby you are told a set of principles or facts and you deduce logically some conclusions through proofs. The steps may be tricky and may require deep mathematical understanding. But the important point is that there is a right answer to most of these problems. Data science is much more akin to inductive reasoning. Inductive reasoning involves taking a small set of representative examples (say a sample of data) and trying to generalize these examples to make a broader statement (say a population). Even when the data are correct, the conculions you may draw can be wildly inaccurate. So the hard thing about data science is describing a path from a set of known data to a set of conclusions that can be supported by the data. Ideally, these conclusions will hold up to scrutiny, skepticism, and replication. In other words, data science is hard precisely because there is often not a “right” answer. Good data science is distinguished from bad data science primarily by a repeatable, thoughtful, skeptical application of an analytic process to data in order to arrive at supportable conclusions. Andrew Gelman Paper on Inductive vs Deductive Reasoning 2.2.3 So what is advanced data science? Ask yourselves, what problem have you solved, ever, that was worth solving, where you knew knew all of the given information in advance? Where you didn’t have a surplus of information and have to filter it out, or you didn’t have insufficient information and have to go find some? This quote comes from a Dan Meyer Ted Talk about patient problem solving. But it applies equally to data science. Data science is answering questions with data and it requires a range of skills and therefore a range of classes: Level 0: Background: Basic computing, some calculus with a focus on optimization, basic linear algebra. Level 1: Data science thinking: How to define a question, how to turn a question into a statement about data, how to identify data sets that may be applicable, experimental design, critical thinking about data sets. Level 2: Data science communication: Teaching students how to write about data science, how to express models qualitatively and in mathematical notation, explaining how to interpret results of algorithms/models. Explaining how to make figures. Level 3: Data science tools: Learning the basic tools of R, loading data of various types, reading data, plotting data. Level 4: Real data: Manipulating different file formats, working with “messy” data, trying to organize multiple data sets into one data set. Level 5: Worked examples: Use real data examples, but work them through from start to finish as case studies, don’t make them easy clean data sets, but have a clear path from the beginning of the problem to the end. Level 6: Just the question: Give students a question where you have done a little research to know that it is posisble to get at least some data, but aren’t 100% sure it is the right data or that the problem can be perfectly solved. Part of the learning process here is knowing how to define success or failure and when to keep going or when to quit. Level 7: The student is the scientist: Have the students come up with their own questions and answer them using data. As you move up the hierarchy of data science classes, the emphasis moves away from technological skills and toward synthesis and communication. The hardest part of data science isn't the technology pic.twitter.com/2IslFWrJwa — Caitlin Hudon 👩🏼‍💻 (@beeonaposy) August 26, 2020 This advanced data science course assumes you have background in statistics, programming, and the basics of project management - it will instead focus on synthesizing these tools into a data analysis and communicating the analysis to an audience. We will instead focus on the “hard” part of data science, which is understanding the way to use data to make generalizable statements about the world and communicating those results to others. 2.3 Types of data analytic questions Data can be used to answer many questions, but not all of them. One of the most innovative data scientists of all time said it best. The data may not contain the answer. The combination of some data and an aching desire for an answer does not ensure that a reasonable answer can be extracted from a given body of data. John Tukey Before performing a data analysis the key is to define the type of question being asked. Some questions are easier to answer with data and some are harder. This is a broad categorization of the types of data analysis questions, ranked by how easy it is to answer the question with data. You can also use the data analysis question type flow chart to help define the question type (from the paper What is the question ) The data analysis question type flow chart 2.3.1 Descriptive A descriptive data analysis seeks to summarize the measurements in a single data set without further interpretation. An example is the United States Census. The Census collects data on the residence type, location, age, sex, and race of all people in the United States at a fixed time. The Census is descriptive because the goal is to summarize the measurements in this fixed data set into population counts and describe how many people live in different parts of the United States. The interpretation and use of these counts is left to Congress and the public, but is not part of the data analysis. 2.3.2 Exploratory An exploratory data analysis builds on a descriptive analysis by searching for discoveries, trends, correlations, or relationships between the measurements of multiple variables to generate ideas or hypotheses. An example is the discovery of a four-planet solar system by amateur astronomers using public astronomical data from the Kepler telescope. The data was made available through the planethunters.org website, that asked amateur astronomers to look for a characteristic pattern of light indicating potential planets. An exploratory analysis like this one seeks to make discoveries, but rarely can confirm those discoveries. In the case of the amateur astronomers, follow-up studies and additional data were needed to confirm the existence of the four-planet system. 2.3.3 Inferential An inferential data analysis goes beyond an exploratory analysis by quantifying whether an observed pattern will likely hold beyond the data set in hand. Inferential data analyses are the most common statistical analysis in the formal scientific literature. An example is a study of whether air pollution correlates with life expectancy at the state level in the United States. The goal is to identify the strength of the relationship in both the specific data set and to determine whether that relationship will hold in future data. In non-randomized experiments, it is usually only possible to observe whether a relationship between two measurements exists. It is often impossible to determine how or why the relationship exists - it could be due to unmeasured data, relationships, or incomplete modeling. 2.3.4 Predictive While an inferential data analysis quantifies the relationships among measurements at population-scale, a predictive data analysis uses a subset of measurements (the features) to predict another measurement (the outcome) on a single person or unit. An example is when organizations like FiveThirtyEight.com use polling data to predict how people will vote on election day. In some cases, the set of measurements used to predict the outcome will be intuitive. There is an obvious reason why polling data may be useful for predicting voting behavior. But predictive data analyses only show that you can predict one measurement from another, they don’t necessarily explain why that choice of prediction works. 2.3.5 Causal A causal data analysis seeks to find out what happens to one measurement if you make another measurement change. An example is a randomized clinical trial to identify whether fecal transplants reduces infections due to Clostridium dificile. In this study, patients were randomized to receive a fecal transplant plus standard care or simply standard care. In the resulting data, the researchers identified a relationship between transplants and infection outcomes. The researchers were able to determine that fecal transplants caused a reduction in infection outcomes. Unlike a predictive or inferential data analysis, a causal data analysis identifies both the magnitude and direction of relationships between variables. 2.3.6 Mechanistic Causal data analyses seek to identify average effects between often noisy variables. For example, decades of data show a clear causal relationship between smoking and cancer. If you smoke, it is a sure thing that your risk of cancer will increase. But it is not a sure thing that you will get cancer. The causal effect is real, but it is an effect on your average risk. A mechanistic data analysis seeks to demonstrate that changing one measurement always and exclusively leads to a specific, deterministic behavior in another. The goal is to not only understand that there is an effect, but how that effect operates. An example of a mechanistic analysis is analyzing data on how wing design changes air flow over a wing, leading to decreased drag. Outside of engineering, mechanistic data analysis is extremely challenging and rarely undertaken. 2.4 A data analytic rubric There is often no “right” or “wrong” answer when evaluating a data analysis, but there are some characteristics that separate good analyses from poor analyses. One difficult thing about advanced data science is that these rules have not been formally defined like much of our statistical theory and are not generally agreed on. We are only at the beginning of developing a “theory” of data analysis - we will learn more about these efforts later in the class. You can think of the theory of data analysis more like guidance than hard and fast rules. Roger Peng outlines this idea in his Dean’s lecture which I would encourage you to watch (the key analogy to music begins around 27 minutes in). For now we will use a basic checklist (adapted from the book Elements of Data Analytic Style) when reviewing data analyses. It can be used as a guide during the process of a data analysis, as a rubric for grading data analysis projects, or as a way to evaluate the quality of a reported data analysis. You don’t have to answer every one of these questions for every data analysis, but they are a useful set of ideas ot keep in the back of your mind when reviewing a data analysis. 2.4.1 Answering the question Did you specify the type of data analytic question (e.g. exploration, association causality) before touching the data? Did you define the metric for success before beginning? Did you understand the context for the question and the scientific or business application? Did you record the experimental design? Did you consider whether the question could be answered with the available data? 2.4.2 Checking the data Did you plot univariate and multivariate summaries of the data? Did you check for outliers? Did you identify the missing data code? 2.4.3 Tidying the data Is each variable one column? Is each observation one row? Do different data types appear in each table? Did you record the recipe for moving from raw to tidy data? Did you create a code book? Did you record all parameters, units, and functions applied to the data? 2.4.4 Exploratory analysis Did you identify missing values? Did you make univariate plots (histograms, density plots, boxplots)? Did you consider correlations between variables (scatterplots)? Did you check the units of all data points to make sure they are in the right range? Did you try to identify any errors or miscoding of variables? Did you consider plotting on a log scale? Would a scatterplot be more informative? 2.4.5 Inference Did you identify what large population you are trying to describe? Did you clearly identify the quantities of interest in your model? Did you consider potential confounders? Did you identify and model potential sources of correlation such as measurements over time or space? Did you calculate a measure of uncertainty for each estimate on the scientific scale? 2.4.6 Prediction Did you identify in advance your error measure? Did you immediately split your data into training and validation? Did you use cross validation, resampling, or bootstrapping only on the training data? Did you create features using only the training data? Did you estimate parameters only on the training data? Did you fix all features, parameters, and models before applying to the validation data? Did you apply only one final model to the validation data and report the error rate? 2.4.7 Causality Did you identify whether your study was randomized? Did you identify potential reasons that causality may not be appropriate such as confounders, missing data, non-ignorable dropout, or unblinded experiments? If not, did you avoid using language that would imply cause and effect? 2.4.8 Written analyses Did you describe the question of interest? Did you describe the data set, experimental design, and question you are answering? Did you specify the type of data analytic question you are answering? Did you specify in clear notation the exact model you are fitting? Did you explain on the scale of interest what each estimate and measure of uncertainty means? Did you report a measure of uncertainty for each estimate on the scientific scale? 2.4.9 Figures Does each figure communicate an important piece of information or address a question of interest? Do all your figures include plain language axis labels? Is the font size large enough to read? Does every figure have a detailed caption that explains all axes, legends, and trends in the figure? 2.4.10 Presentations Did you lead with a brief, understandable to everyone statement of your problem? Did you explain the data, measurement technology, and experimental design before you explained your model? Did you explain the features you will use to model data before you explain the model? Did you make sure all legends and axes were legible from the back of the room? 2.4.11 Reproducibility Did you avoid doing calculations manually? Did you create a script that reproduces all your analyses? Did you save the raw and processed versions of your data? Did you record all versions of the software you used to process the data? Did you try to have someone else run your analysis code to confirm they got the same answers? 2.4.12 R packages Did you make your package name “Googleable” Did you write unit tests for your functions? Did you write help files for all functions? Did you write a vignette? Did you try to reduce dependencies to actively maintained packages? Have you eliminated all errors and warnings from R CMD CHECK? 2.5 Your first assignment - deconstructing an analysis Before we leap into doing data analysis we are going to deconstruct some data analyses to help us discover what are the parts that work and don’t in different contexts. Your first assignment will be to read this data analysis about Mortality in the aftermath of Hurricane Maria. You will answer a series of questions, then we will discuss the analysis during class. Write a brief one paragraph summary of the paper highlighting what you consider to be the key parts of the analysis. Who you think the target audience of this paper is? What kind of question (descriptive, exploratory, inferenential, predictive, casual, or mechanistic) is this paper trying to answer? What is the main question the paper is trying to answer? Do you think the data used in the paper are sufficient to answer the question? What are the main sections of the paper? How do they support (or not) the answer to the question? Do you think the analytic methods (plots, summaries, statistical models) are sufficient to answer the question? Do you think it could have been done with simpler methods? Do you think you could reproduce this analysis based on the text? Do you have access to the data, the code, enough of a description of what happened? Why or why not? Do you think that the analysis was done in the order shown in the paper? Do you think the authors only did the analysis shown in the paper? Do you think that the overall analysis makes a convincing point? Why or why not? 2.6 Additional Resources The Elements of Data Analytic Style {data analysis book} The Art of Data Science {data analysis book} Advanced Data Analysis from an Elementary Point of View {methods book} An Introduction to Statistical Learning {methods book} 2.7 Homework Template Repo: https://github.com/advdatasci/homework1 Repo Name: homework1-ind-yourgithubusername Pull Date: 2020/09/07 9:00AM Baltimore Time "]
]
